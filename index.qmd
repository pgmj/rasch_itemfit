---
title: "Detecting item misfit in Rasch models"
author:
  - name: Magnus Johansson
    affiliations:
      - name: RISE Research Institutes of Sweden, Division Built Environment, System Transition
      - name: Centre for Psychiatry Research, Department of Clinical Neuroscience, Karolinska Institutet, & Stockholm Health Care Services, Region Stockholm
    orcid: 0000-0003-1669-592X
keywords:
  - Rasch
  - Psychometrics
  - Item fit
  - Cutoffs
  - Critical values
  - Model fit
abstract: |
  Psychometrics in general have long relied on rule-of-thumb critical values for various goodness of fit metrics. With more powerful personal computers it is both feasible and desirable to use simulation methods to determine appropriate critical cutoff values. This paper illustrates and evaluates the use of an R package for Rasch psychometrics that has implemented functions to simplify the process of determining simulation based cutoff values. Through a series of simulation studies a comparison is made between information weighted conditional item fit ("infit") and item-restscore correlations using Goodman and Kruskal’s $\gamma$. Results indicate the limitations of small samples (n < 500) in correctly detecting item misfit, especially when several items are misfit. Item outfit shows very low performance and should not be used. Conditional infit with simulation based cutoffs performs slightly better than item-restscore with samples below n = 500. Both methods have strongly increased rates of false positives with large samples (n >= 1000). Large samples should use non-parametric bootstrap of subsamples with item-restscore to avoid type-1 errors. Finally, the importance of iterative analyses is emphasized since a situation where several items show underfit will induce seemingly overfit items. Underfit item should be removed one at a time, and a re-analysis conducted for each step to avoid erroneously removing items.
key-points:
  - Rule-of-thumb critical values for goodness of fit metrics should be replaced with simulation based values
  - Sample size affects the detection rate and false positives rate
  - More misfitting items increases false positive rate
  - Mean squared unweighted "outfit" is not useful
date: last-modified
bibliography: references.bib
number-sections: true
repo-url: https://github.com/pgmj/rasch_itemfit
shorttitle: Detecting item misfit in Rasch models
execute:
  warning: false
  message: false
editor_options: 
  chunk_output_type: console
---

# Introduction

This paper presents a series of simulations conducted to evaluate methods to detect of item misfit in Rasch models. First, conditional item infit and outfit will be under scrutiny. Second, item infit will be compared to item-restscore [@kreiner_note_2011;@mueller_iarm_2022]. Third, a bootstrap method for item-restscore will be presented and tested.

The assessment of item fit under the Rasch model has for decades been conducted using various more or less arbitrary rule-of-thumb critical values. Müller [-@muller_item_2020] showed how the range of critical values for conditional item infit varies with sample size. The expected average item conditional infit range was described by Müller as fairly well captured by Smith's rule-of-thumb formula 1±2/$\sqrt{n}$ [@smith_using_1998]. However, the average range does not apply for all items, since item location relative to sample location also affects model expected item fit. This means that some items within a set of items varying in location are likely to have item fit values outside Smith's average value range while still fitting the Rasch model.

It is here proposed that by using parametric bootstrapping one can establish item fit critical cutoff values that are sample and item specific. This procedure uses the estimated item and person locations based on the available data and simulates new response data that fit the Rasch model, to determine the range of plausible item fit values for each item. The R package `easyRasch` [@easyrasch] includes a function to determine item infit and outfit cutoff values using this method and will be tested in the simulations in this paper.

Similar developments have recently taken place in the related field of confirmatory factor analysis. McNeish and Wolf [-@mcneish_direct_2024] have created an R package called `dynamic` that uses simulation to determine appropriate critical values for commonly used model fit metrics, both for ordinal data and interval data.

It is important to note that the conditional item fit described by Müller [-@muller_item_2020] and implemented in the `iarm` R package [@mueller_iarm_2022] should not be confused with the unconditional item fit implemented in software such as Winsteps and RUMM2030, as well as all R packages except `iarm`. Unconditional item fit can result in unreliable item fit in sample sizes as small as 250 with increasing likelihood of problems as sample size increases. Readers are strongly recommended to read Müller's paper to fully understand the issues with unconditional item fit.

# Methods 

```{r}
library(iarm)
library(eRm)
library(ggdist)
library(tidyverse)
library(easyRasch)
library(arrow)
library(showtext)

showtext_auto()

### some commands exist in multiple packages, here we define preferred ones that are frequently used
select <- dplyr::select
count <- dplyr::count
rename <- dplyr::rename

theme_rise <- theme_rise(fontfamily = "sans")
```

A fully reproducible manuscript with R code and data is available on GitHub: <https://github.com/pgmj/rasch_itemfit>

The simulation of response data used three steps: First, a vector of theta values (person scores on the latent variable's logit scale) were generated using `rnorm(mean = 0, sd = 1.5)`. Second, a set of item locations ranging from -2 to 2 logits were generated for dichotomous items, using `runif(n = 20, min = -2, max = 2)`. Third, the theta values were used to simulate item responses for participants, using `sim.xdim()` from the `eRm` package [@mair_extended_2007], which allows simulation of multidimensional response data. Multiple datasets with 10 000 respondents each were generated using the same item and person parameters, varying the targeting of the misfitting item(s) and number of the misfitting item(s). More details are described under the separate studies. The parametric bootstrapping procedure was implemented using random samples from the simulated datasets. Sample size variations tested are described under each study. 

The general procedure for the parametric bootstrapping is as follows:

1. Estimation of item locations based on simulated item response data, using conditional maximum likelihood [CML, @mair_extended_2007].
2. Estimation of sample theta values using weighted maximum likelihood [@warm_weighted_1989].
3. Simulation of new response data which fit the Rasch model, using the estimated item locations and theta values.
4. Estimation of the dichotomous Rasch model for the new response data using CML.
5. Based on step 4, calculation of conditional item infit and outfit [@muller_item_2020;@mueller_iarm_2022] and/or item-restscore metrics [@kreiner_note_2011;@mueller_iarm_2022].

Steps three and four were iterated over, using resampling with replacement from the estimated theta values as a basis for simulating the response data in step three.

Summary statistics were created with focus on the percentage of correct detection of misfit and false positives.

A complete list of software used for the analyses is listed in #sec-addmat.

```{r}
# read pre-generated item locations for reproducibility
items1 <- read_csv("data/rm_items40.csv") %>%
  slice(1:20) %>% 
  pull(location)
```

```{r}
#| eval: false
# a matrix to specify which dimension each item loads on
wmat0 <- matrix(nrow = 20,
               ncol = 2)
# set all items to load on dimension one
wmat0[1:20,1] <- 1
wmat0[1:20,2] <- 0
# item 9, with good targeting in this item set (closest to sample theta mean of 0), is chosen to belong to a second dimension
wmat0[9,1] <- 0
wmat0[9,2] <- 1

# a matrix to specify which dimension each item loads on
wmat1 <- matrix(nrow = 20,
               ncol = 2)
wmat1[1:20,1] <- 1
wmat1[1:20,2] <- 0
wmat1[18,1] <- 0
wmat1[18,2] <- 1

# a matrix to specify which dimension each item loads on
wmat2 <- matrix(nrow = 20,
               ncol = 2)
wmat2[1:20,1] <- 1
wmat2[1:20,2] <- 0
wmat2[13,1] <- 0
wmat2[13,2] <- 1

# generate dichotomous data
simdata0 <- eRm::sim.xdim(10000, items1, cutpoint = "randomized", Sigma = sigma, weightmat = wmat0)
simdata1 <- eRm::sim.xdim(10000, items1, cutpoint = "randomized", Sigma = sigma, weightmat = wmat1)
simdata2 <- eRm::sim.xdim(10000, items1, cutpoint = "randomized", Sigma = sigma, weightmat = wmat2)

simdata <- list(data0 = simdata0,
                data1 = simdata1,
                data2 = simdata2)
#saveRDS(simdata,"simdata10000.rds")

### And one dataset with all 3 items misfitting
# a matrix to specify which dimension each item loads on
wmat3 <- matrix(nrow = 20,
               ncol = 2)
# set all items to load on dimension one
wmat3[1:20,1] <- 1
wmat3[1:20,2] <- 0
# item 9, with good targeting in this item set (closest to sample theta mean of 0), is chosen to belong to a second dimension
wmat3[c(9,13,18),1] <- 0
wmat3[c(9,13,18),2] <- 1

# generate dichotomous data
simdata3 <- eRm::sim.xdim(10000, items1, cutpoint = "randomized", Sigma = sigma, weightmat = wmat3)

#saveRDS(simdata3,"data/simdata3.rds")
```

```{r}
#| eval: false
# simulate data using 40 items
# read pre-generated item locations for reproducibility
items2 <- read_csv("data/rm_items40.csv") %>%
  pull(location)

# a matrix to specify which dimension each item loads on
wmat4 <- matrix(nrow = 40,
               ncol = 2)
# set all items to load on dimension one
wmat4[1:40,1] <- 1
wmat4[1:40,2] <- 0

wmat4[c(9,13,18),1] <- 0
wmat4[c(9,13,18),2] <- 1

# generate dichotomous data
simdata40items <- eRm::sim.xdim(10000, items2, cutpoint = "randomized", Sigma = sigma, weightmat = wmat4)

# check targeting
#RItargeting(as.data.frame(simdata40items), model = "RM")
#RItargeting(simdata3[,1:10], model = "RM")

#saveRDS(simdata40items,"data/simdata40items.rds")
```

```{r}
simdata <- readRDS("data/simdata10000.rds")

simdata3 <- readRDS("data/simdata3.rds") %>% 
  as.data.frame()

demodata <- simdata[[1]] %>% 
  as.data.frame() %>% 
  slice(1:400) %>% 
  select(V1,V11,V3,V12)

simdata40items <- readRDS("data/simdata40items.rds") %>% 
  as.data.frame()
```

# Study 1: Item infit and outfit

Item mean square standardized residuals are either unweighted, which is referred to as "outfit", or information weighted, which we call "infit" [@ostini_polytomous_2006, pp. 86-87]. For details on conditional item fit we refer to the previously mentioned paper by Müller [-@muller_item_2020]. Conditional item infit and outfit are expected to be near 1, with higher values indicating an item to be underfitting the Rasch model (often due to multidimensionality issues) and lower values indicating overfit.

The function `RIgetfit()` from the `easyRasch` R package is tested here. It's source code can be accessed on GitHub, see #sec-addmat. The function offers the user a choice of the number of bootstrap iterations to use to determine the critical cutoff values for each item's infit and outfit. Our main interest in this study is two-fold. We want to test variations in the number of iterations used in `RIgetfit()` and evaluate how well the critical values based on the parametric bootstrapping procedure detects misfitting items. Additionally, a comparison between infit and outfit statistics in terms of detection rate and false positive rate will be conducted.

20 dichotomous items are used, with one item misfitting. Item locations are the same throughout all studies unless otherwise noted. The location of the misfitting item relative the to the sample theta mean was selected to be approximately 0, -1, and -2 logits. Three separate datasets were generated with these variations, each with 10 000 simulated respondents. One dataset with all three misfitting items was also generated, using the same sample size.

Then the `RIitemfit()` function is used to summarize the bootstrap results and also calculates the infit and outfit for each item in the observed data and highlights items with infit/outfit values outside of the cutoff values. `RIitemfit()` has a default (user modifiable) setting to slightly truncate the distribution of values using `stats::quantile()` at 0.001 and 0.999 to remove extreme values. An example is demonstrated in @tbl-itemfit1, using a subset of the items used in the simulations. @fig-itemfit1 provides a visualization of the distribution of bootstrapped infit and outfit values, together with the infit/outfit values from the observed data illustrated using an orange diamond shape. Note the variation between items in plausible values of infit and outfit based on the bootstrap, and that Smith's rule-of-thumb regarding infit (1±2/$\sqrt{n}$) would be 0.9-1.1 for a sample size of 400.

This study was rather computationally demanding since each simulation run entailed 100-400 underlying bootstrap iterations. The sample sizes used were 150, 250, 500, and 1000. The number of iterations to determine cutoff values were 100, 200, and 400. Sample size and iteration conditions were fully crossed with each other and the three different targeting variations of the one misfitting item, resulting in 4*3*3 = 36 conditions. Each combination used 200 simulation runs. The simulations took about 12 hours to run on a Macbook Pro Max M1 using 9 CPU cores.

```{r}
#| label: tbl-itemfit1
#| tbl-cap: Conditional item fit with simulation based cutoff values
#| cache: true

simfit <- RIgetfit(demodata, iterations = 400, cpu = 8)
RIitemfit(demodata, simfit, output = "quarto")
```

```{r}
#| label: fig-itemfit1
#| fig-cap: Distribution of simulation based item fit and estimated item fit from observed data
RIgetfitPlot(simfit, demodata)
```


```{r}
# define function to run simulations for item infit/outfit cutoff values
itemfitboot <- function(dat, iterations, samplesize) {
  
  fit <- list()
  fit <- foreach(i = 1:iterations) %do% {
    data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
      as.data.frame()
    
    # check data for responses in all cells
    n_resp <-
      data %>%
      as.matrix() %>%
      colSums2() %>%
      t() %>%
      as.vector()
    
    if (min(n_resp, na.rm = TRUE) < 11) {
      data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
        as.data.frame()
    } 
    
    # get simulation based cutoff values
    sfit100 <- RIgetfit(data,100,9)
    sfit200 <- RIgetfit(data,200,9)
    sfit400 <- RIgetfit(data,400,9)

    # apply cutoffs and store results
    rfit100 <- RIitemfit(data,sfit100, output = "dataframe") %>% 
      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% 
      add_column(item = 1:ncol(data),
                 sims = 100,
                 iteration = i,
                 samplesize = samplesize)
    
    rfit200 <- RIitemfit(data,sfit200, output = "dataframe") %>% 
      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% 
      add_column(item = 1:ncol(data),
                 sims = 200,
                 iteration = i,
                 samplesize = samplesize)
    
    rfit400 <- RIitemfit(data,sfit400, output = "dataframe") %>% 
      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% 
      add_column(item = 1:ncol(data),
                 sims = 400,
                 iteration = i,
                 samplesize = samplesize)
    
    # combine output 
    fit <- rbind(rfit100,rfit200,rfit400)
  }
  return(fit)
}

```

```{r}
#| eval: false
samplesizes <- c(150,250,500,1000)

ifb0 <- list()
ifb1 <- list()
ifb2 <- list()

#library(tictoc)
#tic()
ifb0 <- map(samplesizes, ~ itemfitboot(simdata[[1]], iterations = 200, samplesize = .x))
#toc() # 14257.358 sec elapsed

ifb1 <- map(samplesizes, ~ itemfitboot(simdata[[2]], iterations = 200, samplesize = .x))
ifb2 <- map(samplesizes, ~ itemfitboot(simdata[[3]], iterations = 200, samplesize = .x))

# saveRDS(ifb0, "data/ifb0_200.rds")
# saveRDS(ifb2, "data/ifb2_200.rds")
# saveRDS(ifb1, "data/ifb1_200.rds")

ifb0_df <- map_dfr(1:4, ~ do.call("rbind", ifb0[[.x]])) %>% 
  add_column(targeting = 0)

ifb1_df <- map_dfr(1:4, ~ do.call("rbind", ifb1[[.x]])) %>% 
  add_column(targeting = 1)

ifb2_df <- map_dfr(1:4, ~ do.call("rbind", ifb2[[.x]])) %>% 
  add_column(targeting = 2)

ifb <- rbind(ifb0_df,ifb1_df,ifb2_df)

write_parquet(ifb,"data/ifb.parquet")
```


## Results

```{r}
ifb <- read_parquet("data/ifb.parquet")
```

Figures show the percent of simulation runs that have identified an item as misfitting. Items with more than 5% are colored in light red. A number representing the detection rate is shown adjacent to the bar representing the misfitting item. The figure grid columns are labelled with the number of iterations used by `RIgetfit()` to determine cutoff values, and grid rows are labelled with the sample size.

### Infit

```{r}
#| label: fig-ifb0
#| fig-cap: Conditional infit detection rate (misfit item at 0 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!infit_diff == "no misfit",
         targeting == 0) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
  ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting 0 logits (item 9 misfit). 200 simulations per combination.",
       title = "Conditional infit detection rate")
```

@fig-ifb0 shows the detection rate when the misfitting item is located at the sample mean. Detection rate is highest for the condition with 100 iterations with sample size 150 and 250, but it also shows higher levels of false positives when sample size increases to 500 or more.

```{r}
#| label: fig-ifb1
#| fig-cap: Conditional infit detection rate (misfit item at -1 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!infit_diff == "no misfit",
         targeting == 1) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
  ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = 1.2, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -1 logits (item 18 misfit). 200 simulations per combination.",
       title = "Conditional infit detection rate")
```

When the misfitting item is offset in targeting by -1 logits compared to the sample mean (see @fig-ifb1), the smallest sample size has less power to detect misfit compared to the on-target misfitting item. There are lower rates of false positives across all sample sizes and iterations.

```{r}
#| label: fig-ifb2
#| fig-cap: Conditional infit detection rate (misfit item at -2 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!infit_diff == "no misfit",
         targeting == 2) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
  ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = 1.2, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -2 logits (item 13 misfit). 200 simulations per combination.",
       title = "Conditional infit detection rate")
```

Finally, when the misfitting item is located at -2 logits compared to the sample mean (see @fig-ifb2), we see a stronger reduction in power for sample sizes 150 and 250. No false positives are identified.

### Outfit

```{r}
#| label: fig-ifb0out
#| fig-cap: Conditional outfit detection rate (misfit item at 0 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!outfit_diff == "no misfit",
         targeting == 0) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
    ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = 0, vjust = -1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting 0 logits (item 9 misfit). 200 simulations per combination.",
       title = "Conditional outfit detection rate")
```

```{r}
#| label: fig-ifb1out
#| fig-cap: Conditional outfit detection rate (misfit item at -1 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!outfit_diff == "no misfit",
         targeting == 1) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
    ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = 1.2, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -1 logits (item 18 misfit). 200 simulations per combination.",
       title = "Conditional outfit detection rate")
```

```{r}
#| label: fig-ifb2out
#| fig-cap: Conditional outfit detection rate (misfit item at -2 logits)
#| fig-height: 5
ifb %>% 
  group_by(targeting, samplesize, sims) %>% 
  filter(!outfit_diff == "no misfit",
         targeting == 2) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
    ungroup() %>% 
  mutate(sims = factor(sims, levels = c(100,200,400),
                       labels = c("100 iterations",
                                  "200 iterations",
                                  "400 iterations")),
         samplesize = factor(samplesize, levels = c(150,250,500,1000),
                       labels = c("n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~sims) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -2 logits (item 13 misfit). 200 simulations per combination.",
       title = "Conditional outfit detection rate")
```

As shown in @fig-ifb0out, @fig-ifb1out, and @fig-ifb2out, outfit is performing worse than infit across the board.

### Comments

Based on these simulation, it is highly recommended to use infit over outfit in assessing item fit. The performance of outfit calls to question whether it is useful at all for detecting item misfit.

Regarding infit and the use of parametric bootstrapping with the function `RIgetfit()`, it looks like 100 iterations are to recommend to determine cutoff values when the sample size is 250 or lower, while 200 or 400 iterations reduce the risk for false positives at sample sizes of 500 or larger. False positives are found at sample sizes 500 and 1000 only. The risk for false positives is notably higher when the misfitting item is located at the sample mean compared to when the misfitting item is off-target by -1 logits or more.


# Study 2: Item-restscore

Item-restscore is a metric that compares an expected correlation with the observed correlation, using Goodman and Kruskal’s $\gamma$ [@goodman_measures_1954;@kreiner_note_2011]. Lower observed values than expected indicates than an item is underfit to the Rasch model, while higher values indicate overfit. The item-restscore function used in this simulation is from the `iarm` package [@mueller_iarm_2022] and outputs Benjamini-Hochberg corrected *p*-values [@benjamini_controlling_1995], which are used to determine whether the differences between the observed and expected values are statistically significant (using *p* < .05 as critical value) for each item.

```{r}
ir <- function(dat, iterations, samplesize, cpu = 9) {
  
  require(doParallel)
  registerDoParallel(cores = cpu)
  
  fit <- data.frame()
  fit <- foreach(i = 1:iterations, .combine = rbind) %dopar% {
    data <- dat[sample(1:nrow(dat), samplesize), ]
    
    erm_out <- RM(data)
    
    cfit <- out_infit(erm_out)
    cfit_d <- data.frame(infit = cfit$Infit, outfit = cfit$Outfit) %>% 
      round(3)
    
    i1 <- item_restscore(erm_out)
    i1 <- as.data.frame(i1)
    
    i1d <- data.frame("observed" = as.numeric(i1[[1]][1:ncol(data),1]),
                     "expected" = as.numeric(i1[[1]][1:ncol(data),2]),
                     "se" = as.numeric(i1[[1]][1:ncol(data),3]),
                     "p.value" = as.numeric(i1[[1]][1:ncol(data),4]),
                     "p.adj.BH" = as.numeric(i1[[1]][1:ncol(data),5])
    ) %>% 
      mutate(diff_abs = abs(expected - observed),
             diff = expected - observed,
             ir_padj = ifelse(p.adj.BH < .05, "sign. misfit","no misfit")) %>% 
      select(ir_padj, diff, diff_abs) %>% 
      mutate(item = 1:ncol(data)) %>% 
      add_column(iteration = i,
                 samplesize = samplesize)
    cbind(i1d,cfit_d)
  }
  return(fit)
}

```

```{r}
#| eval: false
samplesizes <- c(100,150,250,500,1000)

ir0 <- list()
ir1 <- list()
ir2 <- list()
#library(tictoc)
#tic()
ir0 <- map(samplesizes, ~ ir(simdata[[1]], iterations = 1000, samplesize = .x))
ir1 <- map(samplesizes, ~ ir(simdata[[2]], iterations = 1000, samplesize = .x))
ir2 <- map(samplesizes, ~ ir(simdata[[3]], iterations = 1000, samplesize = .x))
#toc()
# 483.373 sec elapsed

irall <- list(ir0,ir1,ir2)

#saveRDS(irall,"data/item_restscore1000.rds")
```

## Results
```{r}
#| label: fig-itemrestscore1
#| fig-cap: Item-restscore detection rate across targeting and sample size
#| fig-height: 6

ir_all <- readRDS("data/item_restscore1000.rds")
ir_results <- bind_rows(ir_all[[1]],ir_all[[2]],ir_all[[3]]) %>% 
  add_column(targeting = rep(c(0,1,2), each = 100000))

# using patchwork and separate plots to get geom_text working across targeting variations
results0 <- ir_results %>% 
  filter(targeting == 0) %>% 
  group_by(targeting, samplesize) %>% 
  filter(ir_padj == "sign. misfit" ) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/1000,
         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','-1 logit offset','-2 logits offset')),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "lightblue","lightpink"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting 0 logits") +
  theme(strip.text = element_blank())

results1 <- ir_results %>% 
  filter(targeting == 1) %>% 
  group_by(targeting, samplesize) %>% 
  filter(ir_padj == "sign. misfit" ) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/1000,
         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','1 logit offset','2 logits offset')),
         item = factor(item)) %>%  
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "lightblue","lightpink"))) +
  geom_text(data = . %>% filter(item == 18),
            aes(label = n/10), position = position_dodge(width = 0.9),
            hjust = 1.2, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -1 logits") +
  theme(strip.text = element_blank())

results2 <- ir_results %>% 
  filter(targeting == 2) %>% 
  group_by(targeting, samplesize) %>% 
  filter(ir_padj == "sign. misfit" ) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/1000,
         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','1 logit offset','2 logits offset'))) %>% 
  mutate(samplesize = factor(samplesize, levels = c(100,150,250,500,1000),
                       labels = c("n = 100",
                                  "n = 150",
                                  "n = 250",
                                  "n = 500",
                                  "n = 1000")),
         item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "lightblue","lightpink"))) +
  geom_text(data = . %>% filter(item == 13),
            aes(label = n/10), position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Targeting -2 logits")

results0 + results1 + results2 +
  plot_layout(axes = "collect",
              axis_titles = "collect") +
  plot_annotation(title = "Item-restscore detection rate across targeting and sample size",
                  subtitle = "1000 simulated datasets for each combination.",
                  theme = theme_rise(fontfamily = "sans"))
```

This simulation includes an additional condition with 100 respondents, which results in significantly lower detection rates compared to n = 150. Compared to infit at 250 respondents, item-restscore has detection rates of 95.2%, 90.9%, and 62.4% for targeting 0, -1, and -2, while infit has 96.5%, 96.5%, and 71%. For sample size 500 and 1000, detection rate is similar, including the increased tendency for false positives at n = 1000. The false positive rate is lower for item-restscore than infit for sample sizes below 1000.

# Study 3: Comparing infit and item-restscore

We will now compare the performance of infit and item-restscore when all three items are misfitting at the same time. This simulation will also include a condition with 2000 respondents, to examine if the false positive rate increases with more respondents. For infit, we will use 100 iterations with `RIgetfit()` for n < 500, and 200 for n >= 500, since this produced the best results in Study 1. Outfit is also included to see if it performs as bad as with one misfitting item.

```{r}
# define function to run simulations for item infit/outfit cutoff values
itemfitboot2 <- function(dat, iterations, samplesize) {
  
  fit <- list()
  fit <- foreach(i = 1:iterations) %do% {
    data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
      as.data.frame()
    
    # check data for responses in all cells
    n_resp <-
      data %>%
      as.matrix() %>%
      colSums2() %>%
      t() %>%
      as.vector()
    
    if (min(n_resp, na.rm = TRUE) < 11) {
      data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
        as.data.frame()
    } 
    
    if (nrow(data < 400)) {
      sfit <- RIgetfit(data,100,9)
    } else if (nrow(data > 400)) {
      sfit <- RIgetfit(data,200,9)
    }
    
    # apply cutoffs and store results
    rfit <- RIitemfit(data, sfit, output = "dataframe") %>% 
      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% 
      add_column(item = 1:ncol(data),
                 iteration = i,
                 samplesize = samplesize)

    # combine output 
    fit <- rfit
  }
  return(fit)
}

```


```{r}
#| eval: false
samplesizes <- c(150,250,500,1000,2000)
ifb3 <- list()
ir3 <- list()

ifb3 <- map(samplesizes, ~ itemfitboot2(simdata3, iterations = 500, samplesize = .x))
# I initally used 200 iterations for n = 150 and n = 250, which later seemed illogical, so the next line replaces those simulations
ifb3b <- map(c(150,250), ~ itemfitboot2(simdata3, iterations = 500, samplesize = .x))
#saveRDS(ifb3b,"data/ifb3b.rds")

ir3 <- map(samplesizes, ~ ir(simdata3, iterations = 500, samplesize = .x))

#saveRDS(ifb3,"data/ifb3.rds")
#saveRDS(ir3,"data/ir3.rds")
```

```{r}
ifb3 <- readRDS("data/ifb3.rds")
ifb3b <- readRDS("data/ifb3b.rds")

ir3 <- readRDS("data/ir3.rds")

ifb3_df <- bind_rows(ifb3)
ifb3b_df <- bind_rows(ifb3b)

# filter out n = 150-250 from the simulation with 200 iterations and replace with 100 iterations
ifb3_df <- ifb3_df %>% 
  filter(!samplesize %in% c(150,250)) %>% 
  select(!sims)

ifb3_df <- rbind(ifb3_df,ifb3b_df)

ir3_df <- bind_rows(ir3)
```

### Results

```{r}
#| label: fig-ifb3out
#| fig-cap: Conditional outfit detection rate with three misfitting items
#| fig-height: 7
ifb3_df %>% 
  group_by(samplesize) %>% 
  filter(!outfit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/500*100) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000")),
         item = factor(item)) %>% 

  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = -0.5) +
    geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
    geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~.) +
  theme_rise(fontfamily = "sans") +
  labs(str_wrap("3 items misfitting. 200 iterations used to determine critial value. 500 simulations per combination.",68),
       title = "Conditional outfit detection rate")
```

```{r}
#| label: fig-ifb3
#| fig-cap: Conditional infit detection rate with three misfitting items
#| fig-height: 7
ifb3_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/500*100) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000")),
         item = factor(item)) %>% 

  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
    geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
    geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize~.) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = str_wrap("3 items misfitting. 500 simulations per combination.",68),
       title = "Conditional infit detection rate")
```

Looking at the performance of infit with three misfitting items (@fig-ifb3), we can see that the detection rate is markedly worse for item 13 (targeting -2 logits) in sample sizes 500 and below, compared to when single items were misfitting. The false positive rate has increased for sample size of 1000 and we can see it increase strongly at n = 2000. Outfit (@fig-ifb3out) again performs worse than infit.

```{r}
#| label: fig-itemrestscore2
#| fig-height: 7

ir3_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "lightblue","lightpink"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
    geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
    geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.5, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item',  guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "3 items misfitting. 500 simulations per combination.",
       title = "Item-restscore detection rate")

```

```{r}
#| label: fig-comp1
#| fig-cap: Detection rate for item-restscore compared to infit
#| fig-height: 5

ifb3_sum <- ifb3_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(percent = n/500*100) %>% 
  ungroup() %>% 
  filter(item %in% c(9,13,18)) %>% 
  select(!c(n)) %>% 
  add_column(Method = "Infit")

ir3_sum <- ir3_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(percent = n*100/500) %>% 
  ungroup() %>% 
  filter(item %in% c(9,13,18)) %>% 
  select(!n) %>% 
  add_column(Method = "Item-restscore")

rbind(ifb3_sum,ir3_sum) %>% 
  mutate(item = factor(item, levels = c(9,18,13), labels = c("Item 9 (location 0)",
                                                          "Item 18 (location -1)",
                                                          "Item 13 (location -2)"),
                       ordered = T)) %>% 
    mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 

  ggplot(aes(x = item, y = percent, fill = Method)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = paste0(percent,"%"), y = 8), color = "white",
            position = position_dodge(width = 0.9),
            size = 3) +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(y = "Detection rate in percent",
       title = "Detection rate for item-restscore compared to infit") +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  theme(axis.title.x = element_blank())

```

Item-restscore (see @fig-itemrestscore2) shows comparable detection rate to infit and higher levels of false positives. A comparison is made between the two in @fig-comp1, where item-restscore is performing better than infit at detecting the -2 logits off-target item at n = 250, and better across all items for n = 500 and n = 1000. Infit performs better for samples n = 150 and n = 250 (except the item with location -2 logits).

```{r}
#| label: tbl-overunder
#| tbl-cap: Item-restscore summary results across all sample sizes
#| tbl-colwidths: [60,40]
ir3_df %>%
  filter(ir_padj == "sign. misfit") %>% 
  mutate(type = ifelse(diff < 0, "overfit","underfit")) %>% 
  count(item,type) %>% 
  mutate(Percent = n*100/2500) %>% 
  arrange(desc(Percent)) %>% 
  select(!n) %>% 
  set_names(c("Item","Type of misfit","Percent")) %>% 
  knitr::kable()
```

Reviewing the type of misfit identified by item-restscore (see @tbl-overunder), the false positives are all overfitting the Rasch model, except for two instances (out of 2500) indicating underfit for item 12. Items 9, 13, and 18, that were simulated to be misfitting due to loading on a separate dimension, are as expected showing underfit to the Rasch model.

# Study 4: Bootstrapped item-restscore

For our final set of simulations, we will use a non-parametric bootstrap procedure with item-restscore. The difference from the parametric bootstrap is that the non-parametric bootstrap samples with replacement directly from the observed response data. First, based on the above problematic sample size of 2000 when three items are misfitting, we will use the bootstrap function to sample with replacement using n = 800 and 250 bootstrap samples. The function `RIbootRestscore()` from the `easyRasch` package will be used. 

```{r}
#| label: tbl-bootir
#| tbl-cap: Example output from `RIbootRestscore()`
#| tbl-colwidths: [60,40]
#| cache: true

#tic()
simdata3 %>% 
  slice_sample(n = 2000) %>% 
  RIbootRestscore(iterations = 250, samplesize = 800, cpu = 8, output = "dataframe", cutoff = 0) %>% 
  select(item,item_restscore,percent) %>% 
  filter(!item_restscore == "no misfit") %>% 
  arrange(desc(percent)) %>% 
  set_names(c("Item","Item-restscore result","Percent of iterations")) %>% 
  knitr::kable()
#toc()
```

`RIbootRestscore()` is demonstrated using a single sample in @tbl-bootir, where the table is sorted on Percent of iterations. The runtime was around 10-12 seconds using 8 CPU cores on a Macbook Pro M1 Max. In our simulation, we will repeat this procedure 500 times and report the average and standard deviation for the percent indicating misfit for each item.

Second, we will also apply the bootstrapped item-restscore method to sample sizes 150 and 250, using the complete sample for the same bootstrap procedure to see if this produces more useful information than previously tested strategies for identifying misfitting items.

## Results

```{r}
#| eval: false
ir_boot <- function(dat, iterations, samplesize) {
  
  fit <- list()
  fit <- foreach(i = 1:iterations) %do% {
    data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
      as.data.frame()
    
    fit <- RIbootRestscore(data, iterations = 250, samplesize = 800, cpu = 9, output = "dataframe", cutoff = 0) %>% 
      select(item,item_restscore,percent) %>% 
      filter(!item_restscore == "no misfit") %>% 
      add_column(iteration = i)
    
  }
  return(fit)
}

irb0 <- ir_boot(simdata3, 500, 2000)
saveRDS(irb0,"data/irb0.rds")
```

```{r}
#| label: fig-irb0all
#| fig-cap: Item-restscore bootstrap results
#| fig-height: 5
irb0 <- readRDS("data/irb0.rds")

irb0_df <- bind_rows(irb0)

# irb0_df %>% 
#   mutate(item = factor(item, levels = paste0("V",20:1), ordered = T)) %>% 
#   ggplot(aes(x = percent, y = item, color = item_restscore)) +
#   stat_pointinterval(point_interval	= "median_hdci") +
#   theme_rise(fontfamily = "sans") +
#   labs(caption = str_wrap("Point indicates median value and lines are highest-density continuous interval for .66 and .95.",80),
#        x = "Distribution of percent values for item misfit", y = "Item",
#        title = "Item-restscore bootstrap results",
#        subtitle = str_wrap("250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.",70)
#        ) +
#   scale_color_discrete('Type', labels = c("Overfit","Underfit"))

irb0_df %>% 
  mutate(item = factor(item, levels = paste0("V",20:1), ordered = T)) %>% 
  group_by(item,item_restscore) %>% 
  summarise(median = median(percent),
            mean = mean(percent),
            q_lower = quantile(percent, .05),
            q_upper = quantile(percent, .95),
            sd = sd(percent),
            mad = mad(percent),
            se = sd(percent) / sqrt(500),
            ci_upper = median + (1.96*se),
            ci_lower = median - (1.96*se)) %>% 
  
  ggplot(aes(x = median, y = item, color = item_restscore)) +
  geom_point(size = 3.5, shape = 18) +
  geom_segment(aes(x = median - mad, xend = median + mad)) +
  theme_rise(fontfamily = "sans") +
  labs(caption = str_wrap("Point indicates median value and horizontal lines are median absolute deviation.",80),
       x = "Distribution of percent values for item misfit", y = "Item",
       title = "Item-restscore bootstrap results",
       subtitle = str_wrap("250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.",70)
       ) +
  scale_color_discrete('Type', labels = c("Overfit","Underfit"))
```

```{r}
#| label: tbl-irb0mis
#| tbl-cap: Summary statistics for item-restscore bootstrap simulation
#| tbl-subcap: 
#|   - "Misfitting items"
#|   - "False positives"
#| layout-ncol: 1

irb0_df %>% 
  filter(item %in% c("V9","V13","V18")) %>% 
  rename(Item = item) %>% 
  group_by(Item) %>% 
  summarise(Median = median(percent),
            MAD = mad(percent),
            Mean = mean(percent),
            SD = sd(percent),
            `Percentile .05` = quantile(percent, .05)
            ) %>% 
  ungroup() %>% 
  arrange(desc(`Percentile .05`)) %>% 
  mutate_if(is.numeric, round, 1) %>% 
  knitr::kable()

irb0_df %>% 
  filter(!item %in% c("V9","V13","V18"),
         item_restscore == "overfit") %>% 
  rename(Item = item) %>% 
  group_by(Item) %>% 
  summarise(Median = median(percent),
            MAD = mad(percent),
            Mean = mean(percent),
            SD = sd(percent),
            `Percentile .95` = quantile(percent, .95)
            ) %>% 
  ungroup() %>% 
  arrange(desc(`Percentile .95`)) %>% 
  mutate_if(is.numeric, round, 1) %>% 
  knitr::kable()
```

@fig-irb0all shows that there is variation in false positive rate and it is nearly always indicating overfit, while the misfitting items are only indicated as underfit. The summary statistics in @tbl-irb0mis show that there can be quite a bit of variation for false positives, but the clear majority of results are below 50%. 3 items have 95th percentile values above 50, with the highest at 58.8.

## Small sample (n = 150)

We will use 200 simulations to check the performance of the bootstrapped item-restscore function for sample size 150. As an additional experimental condition, we will use both 250 and 500 bootstrap iterations for item-restscore in each simulation.

```{r}
#| eval: false
ir_boot2 <- function(dat, iterations, samplesize) {
  
  fit <- list()
  fit <- foreach(i = 1:iterations) %do% {
    data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
      as.data.frame()
    
        # check data for responses in all cells
    n_resp <-
      data %>%
      as.matrix() %>%
      colSums2() %>%
      t() %>%
      as.vector()
    
    if (min(n_resp, na.rm = TRUE) < 11) {
      data <- dat[sample(1:nrow(dat), samplesize), ] %>% 
        as.data.frame()
    } 
    
    fit1 <- RIbootRestscore(data, iterations = 250, samplesize = nrow(data), cpu = 9, output = "dataframe", cutoff = 0) %>% 
      select(item,item_restscore,percent) %>% 
      filter(!item_restscore == "no misfit") %>% 
      add_column(iteration = i,
                 bootit = 250)
    
    fit2 <- RIbootRestscore(data, iterations = 500, samplesize = nrow(data), cpu = 9, output = "dataframe", cutoff = 0) %>% 
      select(item,item_restscore,percent) %>% 
      filter(!item_restscore == "no misfit") %>% 
      add_column(iteration = i,
                 bootit = 500)
    
    fit <- rbind(fit1,fit2)
    
  }
  return(fit)
}

irb150 <- ir_boot2(simdata3, 200, 150)
saveRDS(irb150,"data/irb150.rds")

# irb250 <- ir_boot2(simdata3, 200, 250)
# saveRDS(irb250,"data/irb250.rds")
```

```{r}
#| label: fig-irboot150
#| fig-height: 6
irb150 <- readRDS("data/irb150.rds")

irb150_df <- bind_rows(irb150)

irb150_df %>% 
  mutate(item = factor(item, levels = paste0("V",20:1), ordered = T),
         bootit = factor(bootit, levels = c(250,500),
                         labels = c("250 bootstrap iterations",
                                    "500 bootstrap iterations"))) %>% 
  group_by(item,item_restscore,bootit) %>% 
  summarise(median = median(percent),
            mean = mean(percent),
            q_lower = quantile(percent, .05),
            q_upper = quantile(percent, .95),
            sd = sd(percent),
            mad = mad(percent),
            se = sd(percent) / sqrt(500),
            ci_upper = median + (1.96*se),
            ci_lower = median - (1.96*se)) %>% 
  
  ggplot(aes(x = median, y = item, color = item_restscore)) +
  geom_point(size = 3.5, shape = 18) +
  geom_segment(aes(x = median - mad, xend = median + mad)) +
    geom_text(data = . %>% filter(item == "V9",
                                  item_restscore == "underfit"),
            aes(label = median), color = "black",
            hjust = -0.1, vjust = -1) +
    geom_text(data = . %>% filter(item == "V13",
                                  item_restscore == "underfit"),
            aes(label = median), color = "black",
            hjust = -0.1, vjust = -1) +
    geom_text(data = . %>% filter(item == "V18",
                                  item_restscore == "underfit"),
            aes(label = median), color = "black",
            hjust = -0.1, vjust = -1) +
  theme_rise(fontfamily = "sans") +
  labs(caption = str_wrap("Point indicates median value and horizontal lines are median absolute deviation.",80),
       x = "Distribution of percent values for item misfit", y = "Item",
       title = "Item-restscore bootstrap results",
       subtitle = str_wrap("250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.",70)
       ) +
  scale_color_discrete('Type', labels = c("Overfit","Underfit")) +
  facet_wrap(~bootit)

# irb150_df %>% 
#   mutate(bootit = factor(bootit, levels = c(250,500),
#                          labels = c("250 bootstrap iterations",
#                                     "500 bootstrap iterations"))) %>% 
#   ggplot(aes(x = percent, y = item, color = item_restscore)) +
#   stat_pointinterval(point_interval	= "median_hdci") +
#   theme_rise(fontfamily = "sans") +
#   labs(caption = str_wrap("Point indicates median value and lines are highest-density continuous interval for .66 and .95.",80),
#        x = "Distribution of percent values for item misfit", y = "Item",
#        title = "Item-restscore bootstrap results",
#        subtitle = str_wrap("250 and 500 bootstrap iterations with 150 respondents from a sample of 150. 200 simulations were used.",70)
#        ) +
#   scale_color_discrete('Type', labels = c("Overfit","Underfit")) +
#   facet_wrap(~bootit)
```

Item-restscore bootstrapping improves slightly on the single instance of item-restscore for the n = 150 condition (see @fig-irboot150). When comparing to the previous results in @fig-itemrestscore2, where the detection rate for the same sample size were at 49.2%, 14.6%, and 34.6% (for items 9, 13, and 18 respectively), the corresponding median values from the bootstrapped item-restscore with 250 iterations were 52.4%, 19.2%, and 38.4%. Using 500 bootstrap iterations did not result in relevant improvements over 250 iterations (see @tbl-irb150mis). Compared to the results using infit (@fig-ifb3), with detection rates of 59.2%, 19%, and 51.8%, item-restscore inferior also when bootstrapped.

```{r}
#| label: tbl-irb150mis
#| tbl-cap: Summary statistics for item-restscore bootstrap simulation (n = 150)

irb150_tbl <- irb150_df %>% 
  filter(item %in% c("V9","V13","V18"),
         item_restscore == "underfit") %>% 
  rename(Item = item) %>% 
  group_by(Item,bootit) %>% 
  summarise(Median = median(percent),
            MAD = mad(percent),
            Mean = mean(percent),
            SD = sd(percent)
            ) %>% 
  ungroup() %>% 
  arrange(bootit,desc(Median)) %>% 
  mutate_if(is.numeric, round, 1) %>% 
  mutate(bootit = as.character(bootit)) %>% 
  relocate(bootit, .before = "Item") %>% 
  rename(`Bootstrap iterations` = bootit)  

irb150_tbl[c(1,3,4,6),1] <- ""

knitr::kable(irb150_tbl)
```


# Study 5: Varying number of items 

When doing simulation studies there is always a balance to strike between trying to evaluate many scenarios and not having too high complexity. We have been keeping several things constant, such as item locations and number of items, which makes interpretation easier but may limit the applicability of the results. For our final simulation, we will vary the number of items and the number of misfitting items. First, 40 dichotomous items will be used, adding 20 new item locations to the previously used set, with the same three items misfitting (items 9, 13, and 18). Second, items 1-10 out of the initial 20 items will be used, which means only item 9 will be misfit. We'll again be using sample sizes of 150, 250, 500, and 1000.

Item-restscore and item infit will be compared. The latter will use 100 bootstrap iterations to determine critical values for sample sizes 150 and 250, and 200 bootstrap iterations for n >= 500.

```{r}
#| eval: false

samplesizes <- c(150,250,500,1000)
ifb10 <- list()
ir10 <- list()

#tic()
ifb40_60 <- map(samplesizes, ~ itemfitboot2(simdata40items, iterations = 60, samplesize = .x))
#toc() # 120s for 2 runs
saveRDS(ifb40_60, "data/ifb40_60i.rds")

ifb40_140 <- map(samplesizes, ~ itemfitboot2(simdata40items, iterations = 140, samplesize = .x))
#toc() # 120s for 2 runs
saveRDS(ifb40_140, "data/ifb40_140i.rds")

ifb10 <- map(samplesizes, ~ itemfitboot2(simdata3[,1:10], iterations = 200, samplesize = .x))

saveRDS(ifb10, "data/ifb10.rds")
```


```{r}
#| eval: false
ir40 <- map(samplesizes, ~ ir(simdata40items, iterations = 500, samplesize = .x))
saveRDS(ir40, "data/ir40.rds")

ir10 <- map(samplesizes, ~ ir(simdata3[,1:10], iterations = 500, samplesize = .x))
saveRDS(ir10, "data/ir10.rds")
```

```{r}
ir40 <- readRDS("data/ir40.rds")
ir10 <- readRDS("data/ir10.rds")

ifb40_60 <- readRDS("data/ifb40_60i.rds")
ifb40_140 <- readRDS("data/ifb40_140i.rds")
ifb10 <- readRDS("data/ifb10.rds")


ir40_df <- bind_rows(ir40)
ir10_df <- bind_rows(ir10)

ifb40_60_df <- bind_rows(ifb40_60)
ifb40_140_df <- bind_rows(ifb40_140)
ifb40_df <- rbind(ifb40_60_df,ifb40_140_df)

ifb10_df <- bind_rows(ifb10)

```

## Results 40 items

```{r}
#| label: fig-ifb40
#| fig-height: 5
ifb40_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = round(n/200*100,1)) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 

  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
    geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
    geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_continuous('Item', limits = c(1,40), breaks = seq(1,40,1), guide = guide_axis(n.dodge = 2), minor_breaks = NULL) +
  guides(fill = "none") +
  facet_grid(samplesize~.) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "3 items misfitting. 200 simulations per combination.",
       title = "Conditional infit detection rate")
```


```{r}
#| label: fig-itemrestscore40
#| fig-height: 5

ir40_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "lightblue","lightpink"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
    geom_text(data = . %>% filter(item == 13),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
    geom_text(data = . %>% filter(item == 18),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = -0.3, vjust = 1) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item', 
                     guide = guide_axis(n.dodge = 2)) +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "3 items misfitting. 500 simulations per combination.",
       title = "Item-restscore detection rate")

```

Infit performs better when sample size is 150 or 250 (see @fig-ifb40), while performance is slightly better for item-restscore for n >= 500 in terms of lower rates of false positives (see @fig-itemrestscore40).

## Results 10 items

```{r}
#| label: fig-ifb10
#| fig-height: 5
ifb10_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = round(n/200*100,1)) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000"))) %>% 
  mutate(item = factor(item)) %>% 

  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), 
            position = position_dodge(width = 0.9),
            hjust = 0, vjust = 2) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete('Item') +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  coord_cartesian(clip = "off") +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Item 9 misfitting (targeting = 0). 200 simulations per combination.",
       title = "Conditional infit detection rate")
```

```{r}
#| label: fig-itemrestscore10
#| fig-height: 5

ir10_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  
  ggplot(aes(x = item, y = Percent)) +
  geom_col(aes(fill = ifelse(Percent > 5, "a","b"))) +
  geom_text(data = . %>% filter(item == 9),
            aes(label = Percent), position = position_dodge(width = 0.9),
            hjust = 0, vjust = 2) +
  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +
  scale_x_discrete() +
  guides(fill = "none") +
  facet_grid(samplesize ~ .) +
  theme_rise(fontfamily = "sans") +
  labs(subtitle = "Item 9 misfitting (targeting = 0). 500 simulations per combination.",
       title = "Item-restscore detection rate",
       x = "Item")
```


## Summary figure

```{r}
ifb20sum <- ifb3_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = round(n/500*100,1)) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000"))) %>% 
  mutate(item = factor(item)) %>% 
  select(!n) %>% 
  add_column(n_items = 20)

ifb40sum <- ifb40_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = round(n/200*100,1)) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000"))) %>% 
  mutate(item = factor(item)) %>% 
  select(!n) %>% 
  add_column(n_items = 40)

ifb_k <- rbind(ifb20sum,ifb40sum) %>% 
  add_column(Method = "Conditional infit")



ir20sum <- ir3_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 
  select(!n) %>% 
  add_column(n_items = 20)

ir40sum <- ir40_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 
  select(!n) %>% 
  add_column(n_items = 40)

ir_k <- rbind(ir20sum,ir40sum) %>% 
  add_column(Method = "Item-restscore")

dfk2040 <- rbind(ifb_k,ir_k)
```


```{r}
# 10 and 20 items separately with one misfitting item
ir10sum <- ir10_df %>% 
  group_by(samplesize) %>% 
  filter(ir_padj == "sign. misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/500) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000"))) %>% 
  select(!n) %>% 
  add_column(n_items = 10)

ir20sum2 <- ir_results %>% 
  filter(targeting == 0,
         !samplesize == 100) %>% 
  group_by(targeting, samplesize) %>% 
  filter(ir_padj == "sign. misfit" ) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n*100/1000) %>% 
  ungroup() %>% 
  mutate(item = factor(item)) %>% 
  select(!c(targeting,n)) %>% 
    mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000",
                                        "n = 2000"))) %>% 
  add_column(n_items = 20)


ifb10sum <- ifb10_df %>% 
  group_by(samplesize) %>% 
  filter(!infit_diff == "no misfit") %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = round(n/200*100,1)) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000"))) %>% 
  mutate(item = factor(item)) %>% 
  select(!n) %>% 
  add_column(n_items = 10)

ifb20_1 <- ifb %>% 
  filter(!infit_diff == "no misfit",
         targeting == 0) %>% 
  filter(samplesize %in% c(150,250) & sims == 100) %>% 
  group_by(samplesize) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000")),
         item = factor(item)) %>% 
  select(!n) %>% 
  add_column(n_items = 20)

ifb20_2 <- ifb %>% 
  filter(!infit_diff == "no misfit",
         targeting == 0) %>% 
  filter(samplesize %in% c(500,1000) & sims == 200) %>% 
  group_by(samplesize) %>% 
  count(item, .drop = F) %>% 
  mutate(Percent = n/200*100) %>% 
  ungroup() %>% 
  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),
                             labels = c("n = 150",
                                        "n = 250",
                                        "n = 500",
                                        "n = 1000")),
         item = factor(item)) %>% 
  select(!n) %>% 
  add_column(n_items = 20)

dfk10 <- rbind(ifb10sum,ifb20_1,ifb20_2) %>% 
  add_column(Method = "Conditional infit")

dfk10_2 <- rbind(ir10sum,ir20sum2) %>% 
        add_column(Method = "Item-restscore")


dfk10 <- rbind(dfk10,dfk10_2)
```

```{r}
#| label: fig-comp10
#| fig-cap: Detection rate for item-restscore and infit for 10 items
#| fig-height: 5
dfk10 %>% 
  mutate(item = as.numeric(item)) %>% 
  filter(item %in% c(9),
         !samplesize %in% c("n = 2000")) %>% 
  drop_na(samplesize) %>% 
  mutate(item = factor(item, levels = c(9,18,13), labels = c("Item 9 (location 0)",
                                                             "Item 18 (location -1)",
                                                             "Item 13 (location -2)"),
                       ordered = T),
         n_items = factor(n_items, levels = c(10,20), labels = c("10 items","20 items"))) %>% 

  ggplot(aes(x = item, y = Percent, fill = Method)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = paste0(Percent,"%"), y = 8), color = "white",
            position = position_dodge(width = 0.9),
            size = 3) +
  facet_grid(samplesize ~ n_items) +
  theme_rise(fontfamily = "sans") +
  labs(y = "Detection rate in percent",
       title = "Detection rate for item-restscore and infit across sample sizes",
       subtitle = "One item misfitting with location = 0 logits compared to sample mean.") +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  theme(axis.title.x = element_blank())
  
```

```{r}
#| label: fig-comp2040
#| fig-cap: Detection rate for item-restscore and infit for 20 and 40 items
#| fig-height: 5
dfk2040 %>% 
  mutate(item = as.numeric(item)) %>% 
  filter(item %in% c(9,18,13),
         !samplesize %in% c("n = 2000")) %>% 
  drop_na(samplesize) %>% 
  mutate(item = factor(item, levels = c(9,18,13), labels = c("Item 9 (location 0)",
                                                          "Item 18 (location -1)",
                                                          "Item 13 (location -2)"),
                       ordered = T),
         n_items = factor(n_items, levels = c(20,40), labels = c("20 items","40 items"))) %>% 

  ggplot(aes(x = item, y = Percent, fill = Method)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = paste0(Percent,"%"), y = 8), color = "white",
            position = position_dodge(width = 0.9),
            size = 3) +
  facet_grid(samplesize ~ n_items) +
  theme_rise(fontfamily = "sans") +
  labs(y = "Detection rate in percent",
       title = "Detection rate for item-restscore and infit across sample sizes and number of items",
       subtitle = "Three items misfitting.") +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  theme(axis.title.x = element_blank())
  
```

@fig-comp10 and @fig-comp2040 summarize the findings from the two different comparisons of item numbers.  Adding more items improves the detection rate substantially for both methods, particularly for smaller samples and the off-target items. 40 items compared to 20 items results in a larger improvement for infit over item-restscore for the n = 150 condition, but also the n = 250 and n = 500 conditions for the -2 logits off-target item.

# Discussion

```{r}
#| label: fig-loadloc
#| fig-height: 5
RIloadLoc(simdata3[1:400,], model = "RM")
```

Assessing item fit and dimensionality should be done using multiple methods. Item fit and item-restscore should be used in parallel, while also examining residual patterns by reviewing standardized factor loadings on the first residual contrast (see @fig-loadloc for an example) as well as Yen's Q3 residual correlations [@christensen_critical_2017].

What does this look like with a small sample (@fig-loadloc2) and what is the item fit in the same small sample (@tbl-itemfit2 and @fig-getfit2)?

```{r}
#| label: fig-loadloc2
#| fig-height: 5
RIloadLoc(simdata3[1:150,], model = "RM")
```


```{r}
#| label: tbl-itemfit2
#| cache: true
simfit2 <- RIgetfit(simdata3[1:150,], iterations = 100, cpu = 8)
RIitemfit(simdata3[1:150,], simfit2, output = "quarto")
```


```{r}
#| label: fig-getfit2
#| fig-height: 5
RIgetfitPlot(simfit2, simdata3[1:150,])
```


## Limitations

Number of items could have been varied more and investigated further with more variation in sample sizes.

Partial credit model for polytomous data would have been nice to also test. Although results regarding detection rate should generalize from RM to PCM, maybe the sample size in relation to number of items does not easily translate from the dichotomous case?

iterations for the bootstrapped item-restscore - more testing could be conducted.

# Conclusion

For sample size under 500, rely primarily on item infit with simulation based critical values, using 100 iterations with `RIgetfit()`. For sample sizes closer to 500, item-restscore is recommended, either as a single-run test, or bootstrapped. With samples larger than 500, bootstrapped item-restscore controls false positive rates well, while identifying misfitting items at high detection rates. Using 250 iterations for the bootstrapped item-restscore seems adequate, but more testing could be conducted.

Use both infit and item-restscore in your analysis process, if you have sample size below 1000.

These findings make a good argument for removing one item at a time when the analysis indicates misfitting items, starting with the most underfitting item. This is especially relevant for *n* >= 500 and when misfitting items are located close to the sample mean.




While the simulations in this paper have all used dichotomous data, all functions evaluated in this paper also work with polytomous data using the Rasch Partial Credit Model.

# References {.unnumbered}

:::{#refs}

:::

# Additional materials {#sec-addmat}

- GitHub link for `easyRasch` source code: <https://github.com/pgmj/easyRasch/>
  - Most functions are defined in this file: <https://github.com/pgmj/easyRasch/blob/main/R/easyRasch.R>

## Session info

This documents the specific R packages and versions used in this study. 

```{r}
sessionInfo()
```

