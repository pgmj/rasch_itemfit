
@article{masters_rasch_1982,
	title = {A {Rasch} {Model} for {Partial} {Credit} {Scoring}},
	volume = {47},
	issn = {0033-3123, 1860-0980},
	url = {https://www.cambridge.org/core/journals/psychometrika/article/abs/rasch-model-for-partial-credit-scoring/D7202CB6D2D1593889B1318DD6297E01},
	doi = {10.1007/BF02296272},
	abstract = {A unidimensional latent trait model for responses scored in two or more ordered categories is developed. This “Partial Credit” model is a member of the family of latent trait models which share the property of parameter separability and so permit “specifically objective” comparisons of persons and items. The model can be viewed as an extension of Andrich's Rating Scale model to situations in which ordered response alternatives are free to vary in number and structure from item to item. The difference between the parameters in this model and the “category boundaries” in Samejima's Graded Response model is demonstrated. An unconditional maximum likelihood procedure for estimating the model parameters is developed.},
	language = {en},
	number = {2},
	urldate = {2025-01-12},
	journal = {Psychometrika},
	author = {Masters, Geoff N.},
	month = jun,
	year = {1982},
	keywords = {latent trait, ordered categories, partial credit, Rasch model},
	pages = {149--174},
}


@article{debelak_evaluation_2019,
	title = {An {Evaluation} of {Overall} {Goodness}-of-{Fit} {Tests} for the {Rasch} {Model}},
	volume = {9},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.02710/full},
	doi = {10.3389/fpsyg.2018.02710},
	abstract = {{\textless}p{\textgreater}For assessing the fit of item response theory models, it has been suggested to apply overall goodness-of-fit tests as well as tests for individual items and item pairs. Although numerous goodness-of-fit tests have been proposed in the literature for the Rasch model, their relative power against several model violations has not been investigated so far. This study compares four of these tests, which are all available in R software: {\textless}italic{\textgreater}T{\textless}/italic{\textgreater}$_{\textrm{10}}$, {\textless}italic{\textgreater}T{\textless}/italic{\textgreater}$_{\textrm{11}}$, {\textless}italic{\textgreater}M{\textless}/italic{\textgreater}$_{\textrm{2}}$, and the LR test. Results on the Type I error rate and the sensitivity to violations of different assumptions of the Rasch model (unidimensionality, local independence on the level of item pairs, equal item discrimination, zero as a lower asymptote for the item characteristic curves, invariance of the item parameters) are reported. The results indicate that the {\textless}italic{\textgreater}T{\textless}/italic{\textgreater}$_{\textrm{11}}$ test is comparatively most powerful against violations of the assumption of parallel item characteristic curves, which includes the presence of unequal item discriminations and a non-zero lower asymptote. Against the remaining model violations, which can be summarized as local dependence, {\textless}italic{\textgreater}M{\textless}/italic{\textgreater}$_{\textrm{2}}$ is found to be most powerful. {\textless}italic{\textgreater}T{\textless}/italic{\textgreater}$_{\textrm{10}}$ and LR are found to be sensitive against violations of the assumption of parallel item characteristic curves, but are insensitive against local dependence.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-11},
	journal = {Frontiers in Psychology},
	author = {Debelak, Rudolf},
	month = jan,
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Item fit, Item response theory (IRT), power, Rasch model, Type I error},
	file = {Full Text PDF:/Users/magnuspjo/Zotero/storage/K7UU7MPF/Debelak - 2019 - An Evaluation of Overall Goodness-of-Fit Tests for the Rasch Model.pdf:application/pdf},
}

@article{andersen_goodness_1973,
	title = {A goodness of fit test for the rasch model},
	volume = {38},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02291180},
	doi = {10.1007/BF02291180},
	abstract = {The Rasch model is an item analysis model with logistic item characteristic curves of equal slope,i.e. with constant item discriminating powers. The proposed goodness of fit test is based on a comparison between difficulties estimated from different scoregroups and over-all estimates.},
	language = {en},
	number = {1},
	urldate = {2025-01-11},
	journal = {Psychometrika},
	author = {Andersen, Erling B.},
	month = mar,
	year = {1973},
	keywords = {Asymptotic Covariance Matrix, Conditional Likelihood, Item Difficulty, Item Parameter, Likelihood Equation},
	pages = {123--140},
}

@misc{johansson_simulation_2024,
	title = {Simulation based cutoff values for {Rasch} item fit and residual correlations},
	url = {https://pgmj.github.io/simcutoffs.html},
	language = {en},
	urldate = {2025-01-11},
	journal = {R, Rasch, etc},
	author = {Johansson, Magnus},
	month = sep,
	year = {2024},
}

@book{bond_applying_2015,
	address = {London, UNITED KINGDOM},
	edition = {3rd},
	title = {Applying the {Rasch} {Model}: {Fundamental} {Measurement} in the {Human} {Sciences}},
	isbn = {978-1-315-81469-8},
	shorttitle = {Applying the {Rasch} {Model}},
	abstract = {Cited over 1900 times, this classic text facilitates a deep understanding of the Rasch model. The authors review the crucial properties of the model and demonstrate its use with a variety of examples from education, psychology, and health. A glossary and numerous illustrations aid the reader's understanding. Readers learn how to apply Rasch analysis so they can perform their own analyses and interpret the results. The authors present an accessible overview that does not require a mathematical background. " Highlights of the new edition include: " -More learning tools to strengthen readers understanding including chapter introductions, boldfaced key terms, chapter summaries, activities, and suggested readings. -Divided chapters (4, 6, 7 \& 8) into "basic" and "extended understanding" sections so readers can select the level most appropriate for their needs and to provide more in-depth investigations of key topics. -A website at www.routledge.com/9780415833424 that features free Rasch software, data sets, an Invariance worksheet, detailed instructions for key analyses, and links to related sources. -Greater emphasis on the role of Rasch measurement as "a priori" in the construction of scales and its use "post hoc" to reveal the extent to which interval scale measurement is instantiated in existing data sets. -Emphasizes the importance of interval level measurement data and demonstrates how Rasch measurement is used to examine measurement invariance. -Insights from other Rasch scholars via innovative applications (Ch. 9). -Extended discussion of invariance now reviews DIF, DPF, and anchoring (ch. 5). -Revised Rating Scale Model material now based on the analysis of the CEAQ (ch.6). -Clarifies the relationships between Rasch measurement, True Score Theory, and Item Response Theory by reviewing their commonalities and differences (Ch.13). -Provides more detail on how to conduct a Rasch analysis so readers can use the techniques on their own (Appendix B). Intended as a text for graduate courses in measurement, item response theory, (advanced) research methods or quantitative analysis taught in psychology, education, human development, business, and other social and health sciences, professionals in these areas also appreciate the book s accessible introduction."},
	urldate = {2020-03-11},
	publisher = {Routledge},
	author = {Bond, Trevor and Fox, Christine M.},
	year = {2015},
	keywords = {PSYCHOLOGY / Research \& Methodology},
}

@article{buchardt_visualizing_2023,
	title = {Visualizing {Rasch} item fit using conditional item characteristic curves in {R}},
	volume = {65},
	abstract = {New R computer routines that support rigorous statistical validation of psychological tests have recently appeared. We illustrate how Rasch item fit can be evaluated visually and propose an extension of existing implementations in R. We illustrate the utility using two short psychological tests.},
	language = {en},
	number = {2},
	journal = {Psychological Test and Assessment Modeling},
	author = {Buchardt, Ann-Sophie and Christensen, Karl Bang and Jensen, Normann},
	year = {2023},
	pages = {206--219},
	file = {Buchardt et al. - 2023 - Visualizing Rasch item fit using conditional item .pdf:/Users/magnuspjo/Zotero/storage/I5Q3QZXZ/Buchardt et al. - 2023 - Visualizing Rasch item fit using conditional item .pdf:application/pdf},
}

@article{smith_detecting_2002,
	title = {Detecting and evaluating the impact of multidimensionality using item fit statistics and principal component analysis of residuals},
	volume = {3},
	issn = {1529-7713},
	abstract = {The purpose of this research is twofold. First is to extend the work of Smith (1992, 1996) and Smith and Miao (1991, 1994) in comparing item fit statistics and principal component analysis as tools for assessing the unidimensionality requirement of Rasch models. Second is to demonstrate methods to explore how violations of the unidimensionality requirement influence person measurement. For the first study, rating scale data were simulated to represent varying degrees of multidimensionality and the proportion of items contributing to each component. The second study used responses to a 24 item Attention Deficit Hyperactivity Disorder scale obtained from 317 college undergraduates. The simulation study reveals both an iterative item fit approach and principal component analysis of standardized residuals are effective in detecting items simulated to contribute to multidimensionality. The methods presented in Study 2 demonstrate the potential impact of multidimensionality on norm and criterion-reference person measure interpretations. The results provide researchers with quantitative information to help assist with the qualitative judgment as to whether the impact of multidimensionality is severe enough to warrant removing items from the analysis.},
	language = {eng},
	number = {2},
	journal = {Journal of applied measurement},
	author = {Smith, Everett V},
	month = jan,
	year = {2002},
	pmid = {12011501},
	pages = {205--231},
}


@article{chou_checking_2010,
	title = {Checking {Dimensionality} in {Item} {Response} {Models} {With} {Principal} {Component} {Analysis} on {Standardized} {Residuals}},
	volume = {70},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/0013164410379322},
	doi = {10.1177/0013164410379322},
	abstract = {Dimensionality is an important assumption in item response theory (IRT). Principal component analysis on standardized residuals has been used to check dimensionality, especially under the family of Rasch models. It has been suggested that an eigenvalue greater than 1.5 for the first eigenvalue signifies a violation of unidimensionality when there are 500 persons and 30 items. The cut-point of 1.5 is often used beyond this specific condition of sample size and test length. This study argues that a fixed cut-point is not applicable because the distribution of eigenvalues or their ratios depends on sample size and test length, just like other statistics. The authors conducted a series of simulations to verify this argument. They then proposed three chi-square statistics for multivariate independence to test the correlation matrix obtained from the standardized residuals. Through simulations, it was found that Steiger’s statistic behaved fairly like a chi-square distribution, when its degrees of freedom were adjusted.},
	language = {en},
	number = {5},
	urldate = {2025-01-10},
	journal = {Educational and Psychological Measurement},
	author = {Chou, Yeh-Tai and Wang, Wen-Chung},
	month = oct,
	year = {2010},
	note = {Publisher: SAGE Publications Inc},
	pages = {717--731},
}

@incollection{christensen_item_2013,
	title = {Item {Fit} {Statistics}},
	copyright = {Copyright © 2013 by John Wiley \& Sons, Inc.},
	isbn = {978-1-118-57445-4},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118574454.ch5},
	abstract = {Often it is argued that the parsimonious Rasch model is too simple to have a chance to fit real-life data. Therefore, it is important to provide strong empirical evidence supporting the claim that the Rasch model is adequate for data. This chapter describes two types of item fit statistics that can be used for this purpose. The first type takes all the fundamental assumptions of the Rasch model as given and tries to assess the degree to which the separate items appear to have conditional response probabilities that do not depart from the Rasch model probabilities. The second type addresses the assumption of no differential item functioning (DIF), but does it one item at a time, assuming all the other items do not violate the Rasch model assumptions. The theory of Rasch models uses two types of residuals: individual response residuals and group residuals.},
	language = {en},
	urldate = {2024-08-19},
	booktitle = {Rasch {Models} in {Health}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Christensen, Karl Bang and Kreiner, Svend},
	year = {2013},
	doi = {10.1002/9781118574454.ch5},
	note = {Section: 5
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118574454.ch5},
	keywords = {differential item functioning (DIF), item fit statistics, probability, Rasch model residuals},
	pages = {83--104},
}


@article{christensen_critical_2017,
	title = {Critical {Values} for {Yen}’s {Q3}: {Identification} of {Local} {Dependence} in the {Rasch} {Model} {Using} {Residual} {Correlations}},
	volume = {41},
	issn = {0146-6216},
	shorttitle = {Critical {Values} for {Yen}’s {Q3}},
	url = {https://doi.org/10.1177/0146621616677520},
	doi = {10.1177/0146621616677520},
	abstract = {The assumption of local independence is central to all item response theory (IRT) models. Violations can lead to inflated estimates of reliability and problems with construct validity. For the most widely used fit statistic Q3, there are currently no well-documented suggestions of the critical values which should be used to indicate local dependence (LD), and for this reason, a variety of arbitrary rules of thumb are used. In this study, an empirical data example and Monte Carlo simulation were used to investigate the different factors that can influence the null distribution of residual correlations, with the objective of proposing guidelines that researchers and practitioners can follow when making decisions about LD during scale development and validation. A parametric bootstrapping procedure should be implemented in each separate situation to obtain the critical value of LD applicable to the data set, and provide example critical values for a number of data structure situations. The results show that for the Q3 fit statistic, no single critical value is appropriate for all situations, as the percentiles in the empirical null distribution are influenced by the number of items, the sample size, and the number of response categories. Furthermore, the results show that LD should be considered relative to the average observed residual correlation, rather than to a uniform value, as this results in more stable percentiles for the null distribution of an adjusted fit statistic.},
	language = {en},
	number = {3},
	urldate = {2020-07-12},
	journal = {Applied Psychological Measurement},
	author = {Christensen, Karl Bang and Makransky, Guido and Horton, Mike},
	month = may,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {178--194},
}

@article{mcneish_direct_2024,
	title = {Direct {Discrepancy} {Dynamic} {Fit} {Index} {Cutoffs} for {Arbitrary} {Covariance} {Structure} {Models}},
	volume = {31},
	issn = {1070-5511},
	url = {https://doi.org/10.1080/10705511.2024.2308005},
	doi = {10.1080/10705511.2024.2308005},
	abstract = {Despite the popularity of traditional fit index cutoffs like RMSEA ≤ .06 and CFI ≥ .95, several studies have noted issues with overgeneralizing traditional cutoffs. Computational methods have been proposed to avoid overgeneralization by deriving cutoffs specifically tailored to the characteristics of the model being evaluated. Simulations show favorable performance of these methods; however, these methods support a narrow set of scenarios (e.g., certain models or response scales) and the interpretation of cutoffs is not always standardized, which affects empirical researchers’ ability to confidently and broadly adopt these methods to evaluate model fit. In this paper, we propose an extension to one recently developed computational method—dynamic fit index cutoffs—that (a) permits application to any covariance structure model (e.g., CFA, mediation, bifactor), (b) standardizes interpretation of cutoffs across any covariance structure model, and (c) supports normal, nonnormal, categorical, and missing data. Software is provided to facilitate implementation of the method.},
	number = {5},
	urldate = {2024-10-31},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {McNeish, Daniel and Wolf, Melissa G.},
	month = sep,
	year = {2024},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705511.2024.2308005},
	keywords = {model fit, RMSEA, CFI, cutoffs},
	pages = {835--862},
}


@article{goodman_measures_1954,
	title = {Measures of {Association} for {Cross} {Classifications}},
	volume = {49},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2281536},
	doi = {10.2307/2281536},
	abstract = {When populations are cross-classified with respect to two or more classifications or polytomies, questions often arise about the degree of association existing between the several polytomies. Most of the traditional measures or indices of association are based upon the standard chi-square statistic or on an assumption of underlying joint normality. In this paper a number of alternative measures are considered, almost all based upon a probabilistic model for activity to which the cross-classification may typically lead. Only the case in which the population is completely known is considered, so no question of sampling or measurement error appears. We hope, however, to publish before long some approximate distributions for sample estimators of the measures we propose, and approximate tests of hypotheses. Our major theme is that the measures of association used by an empirical investigator should not be blindly chosen because of tradition and convention only, although these factors may properly be given some weight, but should be constructed in a manner having operational meaning within the context of the particular problem.},
	number = {268},
	urldate = {2025-01-04},
	journal = {Journal of the American Statistical Association},
	author = {Goodman, Leo A. and Kruskal, William H.},
	year = {1954},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {732--764},
}


@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://doi.org/10.1111/j.2517-6161.1995.tb02031.x},
	doi = {10.1111/j.2517-6161.1995.tb02031.x},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses — the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2025-01-03},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	month = jan,
	year = {1995},
	pages = {289--300},
}


@article{smith_using_1998,
	title = {Using item mean squares to evaluate fit to the {Rasch} model},
	volume = {2},
	issn = {1090-655X},
	abstract = {Throughout the mid to late 1970's considerable research was conducted on the properties of Rasch fit mean squares. This work culminated in a variety of transformations to convert the mean squares into approximate t-statistics. This work was primarily motivated by the influence sample size has on the magnitude of the mean squares and the desire to have a single critical value that can generally be applied to most cases. In the late 1980's and the early 1990's the trend seems to have reversed, with numerous researchers using the untransformed fit mean squares as a means of testing fit to the Rasch measurement models. The principal motivation is cited as the influence sample size has on the sensitivity of the t-converted mean squares. The purpose of this paper is to present the historical development of these fit indices and the various transformations and to examine the impact of sample size on both the fit mean squares and the t-transformations of those mean squares. Because the sample size problem has little influence on the person mean square problem, due to the relatively short length (100 items or less), this paper focuses on the item fit mean squares, where it is common to find the statistics used with sample sizes ranging from 30 to 10,000.},
	language = {eng},
	number = {1},
	journal = {Journal of Outcome Measurement},
	author = {Smith, R. M. and Schumacker, R. E. and Bush, M. J.},
	year = {1998},
	pmid = {9661732},
	keywords = {Humans, Psychometrics, Models, Statistical, Statistics as Topic, Reference Standards, Computer Simulation},
	pages = {66--78},
}


@book{ostini_polytomous_2006,
	title = {Polytomous {Item} {Response} {Theory} {Models}},
	isbn = {978-0-7619-3068-6 978-1-4129-8541-3},
	url = {https://methods.sagepub.com/book/polytomous-item-response-theory-models},
	language = {en},
	urldate = {2024-08-19},
	publisher = {SAGE Publications, Inc.},
	author = {Ostini, Remo and Nering, Michael},
	year = {2006},
	doi = {10.4135/9781412985413},
}


@article{muller_item_2020,
	title = {Item fit statistics for {Rasch} analysis: can we trust them?},
	volume = {7},
	issn = {2195-5832},
	shorttitle = {Item fit statistics for {Rasch} analysis},
	url = {https://doi.org/10.1186/s40488-020-00108-7},
	doi = {10.1186/s40488-020-00108-7},
	abstract = {To compare fit statistics for the Rasch model based on estimates of unconditional or conditional response probabilities.},
	number = {1},
	urldate = {2024-06-17},
	journal = {Journal of Statistical Distributions and Applications},
	author = {Müller, Marianne},
	month = aug,
	year = {2020},
	keywords = {Rasch model, Chi-square test statistics, Conditional probability, Outfit and infit statistics},
	pages = {5},
}


@article{kreiner_note_2011,
	title = {A {Note} on {Item}–{Restscore} {Association} in {Rasch} {Models}},
	volume = {35},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621611410227},
	doi = {10.1177/0146621611410227},
	language = {en},
	number = {7},
	urldate = {2024-03-22},
	journal = {Applied Psychological Measurement},
	author = {Kreiner, Svend},
	month = oct,
	year = {2011},
	note = {Publisher: SAGE Publications Inc},
	pages = {557--561},
}


@misc{mueller_iarm_2022,
	title = {iarm: {Item} {Analysis} in {Rasch} {Models}},
	copyright = {GPL-2},
	shorttitle = {iarm},
	url = {https://cran.r-project.org/web/packages/iarm/index.html},
	abstract = {Tools to assess model fit and identify misfitting items for Rasch models (RM) and partial credit models (PCM). Included are item fit statistics, item characteristic curves, item-restscore association, conditional likelihood ratio tests, assessment of measurement error, estimates of the reliability and test targeting as described in Christensen et al. (Eds.) (2013, ISBN:978-1-84821-222-0).},
	urldate = {2024-08-06},
	author = {Mueller, Marianne and Santiago, Pedro Henrique Ribeiro},
	month = aug,
	year = {2022},
	keywords = {Psychometrics},
}

@Manual{easyrasch,
  title = {easyRasch: Psychometric Analysis in R with Rasch Measurement Theory},
  author = {Magnus Johansson},
  year = {2024},
  note = {R package version 0.3.3},
  url = {https://github.com/pgmj/easyRasch},
}


@article{mair_extended_2007,
	title = {Extended {Rasch} {Modeling}: {The} {eRm} {Package} for the {Application} of {IRT} {Models} in {R}},
	volume = {20},
	copyright = {Copyright (c) 2006 Patrick Mair, Reinhold  Hatzinger},
	issn = {1548-7660},
	shorttitle = {Extended {Rasch} {Modeling}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v020i09},
	doi = {10.18637/jss.v020.i09},
	language = {en},
	number = {1},
	urldate = {2020-09-10},
	journal = {Journal of Statistical Software},
	author = {Mair, Patrick and Hatzinger, Reinhold},
	month = feb,
	year = {2007},
	note = {Number: 1},
	pages = {1--20},
}



@article{warm_weighted_1989,
	title = {Weighted likelihood estimation of ability in item response theory},
	volume = {54},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02294627},
	doi = {10.1007/BF02294627},
	abstract = {Applications of item response theory, which depend upon its parameter invariance property, require that parameter estimates be unbiased. A new method, weighted likelihood estimation (WLE), is derived, and proved to be less biased than maximum likelihood estimation (MLE) with the same asymptotic variance and normal distribution. WLE removes the first order bias term from MLE. Two Monte Carlo studies compare WLE with MLE and Bayesian modal estimation (BME) of ability in conventional tests and tailored tests, assuming the item parameters are known constants. The Monte Carlo studies favor WLE over MLE and BME on several criteria over a wide range of the ability scale.},
	language = {en},
	number = {3},
	urldate = {2023-02-01},
	journal = {Psychometrika},
	author = {Warm, Thomas A.},
	month = sep,
	year = {1989},
	pages = {427--450},
}
