{
  "hash": "c78b556e9ba06374320012ac12f30a35",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Detecting item misfit in Rasch models\"\nauthor:\n  - name: Magnus Johansson\n    affiliations:\n      - name: RISE Research Institutes of Sweden, Division Built Environment, System Transition\n      - name: Centre for Psychiatry Research, Department of Clinical Neuroscience, Karolinska Institutet, & Stockholm Health Care Services, Region Stockholm\n    orcid: 0000-0003-1669-592X\nkeywords:\n  - Rasch\n  - Psychometrics\n  - Item fit\n  - Simulation\nabstract: |\n  Psychometrics in general have long relied on rule-of-thumb critical values for various goodness of fit metrics. With more powerful personal computers it is both feasible and desirable to use simulation methods to determine appropriate critical cutoff values. This paper illustrates and evaluates the use of an R package for Rasch psychometrics that has implemented functions to simplify the process of determining simulation based cutoff values. Through a series of simulation studies a comparison is made between information weighted conditional item fit (\"infit\") and item-restscore correlations using Goodman and Kruskal’s $\\gamma$. Results indicate the limitations of small samples (n < 500) in correctly detecting item misfit, especially when several items are misfit. Item outfit shows very low performance and should not be used. Conditional infit with simulation based cutoffs performs slightly better than item-restscore with samples below n = 500. Both methods have strongly increased rates of false positives with large samples (n >= 1000). Large samples should use non-parametric bootstrap of subsamples with item-restscore to avoid type-1 errors. Finally, the importance of iterative analyses is emphasized since a situation where several items show underfit will induce seemingly overfit items. Underfit item should be removed one at a time, and a re-analysis conducted for each step to avoid erroneously removing items.\nkey-points:\n  - Rule-of-thumb critical values for goodness of fit metrics should be replaced with simulation based values\n  - Sample size affects the detection rate and false positives rate\n  - More misfitting items increases false positive rate\n  - Mean squared unweighted \"outfit\" is not useful\ndate: last-modified\nbibliography: references.bib\nnumber-sections: true\nrepo-url: https://github.com/pgmj/rasch_itemfit\nshorttitle: Detecting item misfit in Rasch models\nexecute:\n  warning: false\n  message: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n# Introduction\n\nThis paper presents a series of simulations conducted to evaluate methods to detect of item misfit in Rasch models. First, conditional item infit and outfit will be under scrutiny. Second, item infit will be compared to item-restscore [@kreiner_note_2011;@mueller_iarm_2022]. Third, a bootstrap method for item-restscore will be presented and tested.\n\nThe assessment of item fit under the Rasch model has for decades been conducted using various rule-of-thumb critical values. Müller [-@muller_item_2020] showed how the range of critical values for conditional item infit varies with sample size. The expected average item conditional infit range was described by Müller as fairly well captured by Smith's rule-of-thumb formula 1±2/√n [@smith_using_1998]. However, the average range does not apply for all items, since item location relative to sample location also affects model expected item fit. This means that some items within a set of items varying in location are likely to have item fit values outside Smith's average value range while still fitting the Rasch model.\n\nIt is here proposed that by using parametric bootstrapping one can establish item fit critical cutoff values that are sample and item specific. This procedure uses the estimated item and person locations based on the available data and simulates new response data that fit the Rasch model, to determine the range of plausible item fit values for each item. The R package `easyRasch` [@easyrasch] includes a function to determine item infit and outfit cutoff values using this method and will be tested in the simulations in this paper.\n\nIt is important to note that the conditional item fit described by Müller [-@muller_item_2020] and implemented in the `iarm` R package [@mueller_iarm_2022] should not be confused with the unconditional item fit implemented in software such as Winsteps and RUMM2030, as well as all R packages except `iarm`. Unconditional item fit can result in unreliable item fit in sample sizes as small as 250 with increasing likelihood of problems as sample size increases. Readers are strongly recommended to read Müller's paper to fully understand the issues with unconditional item fit.\n\n# Methods \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(iarm)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: eRm\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(eRm)\nlibrary(ggdist)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(easyRasch)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: formattable\nLoading required package: kableExtra\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nLoading required package: mirt\nLoading required package: stats4\nLoading required package: lattice\n\nAttaching package: 'mirt'\n\nThe following objects are masked from 'package:eRm':\n\n    itemfit, personfit\n\nLoading required package: psych\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nThe following object is masked from 'package:eRm':\n\n    sim.rasch\n\nLoading required package: psychotree\nLoading required package: partykit\nLoading required package: grid\nLoading required package: libcoin\nLoading required package: mvtnorm\nLoading required package: psychotools\nLoading required package: matrixStats\n\nAttaching package: 'matrixStats'\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\nLoading required package: reshape\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nLoading required package: patchwork\n\nAttaching package: 'patchwork'\n\nThe following object is masked from 'package:formattable':\n\n    area\n\nLoading required package: ggrepel\nLoading required package: glue\nLoading required package: catR\nLoading required package: hexbin\nLoading required package: janitor\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nLoading required package: foreach\n\nAttaching package: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\nLoading required package: furrr\nLoading required package: future\nLoading required package: doParallel\nLoading required package: iterators\nLoading required package: parallel\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(arrow)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nSome features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\nThe repository you retrieved Arrow from did not include all of Arrow's features.\nYou can install a fully-featured version by running:\n`install.packages('arrow', repos = 'https://apache.r-universe.dev')`.\n\nAttaching package: 'arrow'\n\nThe following object is masked from 'package:lubridate':\n\n    duration\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(showtext)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: sysfonts\nLoading required package: showtextdb\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nshowtext_auto()\n\n### some commands exist in multiple packages, here we define preferred ones that are frequently used\nselect <- dplyr::select\ncount <- dplyr::count\nrename <- dplyr::rename\n\ntheme_rise <- theme_rise(fontfamily = \"sans\")\n```\n:::\n\n\n\n\n\nA fully reproducible manuscript with R code and data is available on GitHub: <https://github.com/pgmj/rasch_itemfit>\n\nThe simulation of response data used three steps: First, a vector of theta values (person scores on the latent variable's logit scale) were generated using `rnorm(mean = 0, sd = 1.5)`. Second, a set of item locations ranging from -2 to 2 logits were generated for dichotomous items, using `runif(n = 20, min = -2, max = 2)`. Third, the theta values were used to simulate item responses for participants, using `sim.xdim()` from the `eRm` package [@mair_extended_2007], which allows simulation of multidimensional response data. Multiple datasets with 10 000 respondents each were generated using the same item and person parameters, varying the targeting of the misfitting item(s) and number of the misfitting item(s). More details are described under the separate studies. The parametric bootstrapping procedure was implemented using random samples from the simulated datasets. Sample size variations tested are described under each study. \n\nThe general procedure for the parametric bootstrapping is as follows:\n\n1. Estimation of item locations based on simulated item response data, using conditional maximum likelihood [CML, @mair_extended_2007].\n2. Estimation of sample theta values using weighted maximum likelihood [@warm_weighted_1989].\n3. Simulation of new response data which fit the Rasch model, using the estimated item locations and theta values.\n4. Estimation of the dichotomous Rasch model for the new response data using CML.\n5. Based on step 4, calculation of conditional item infit and outfit [@muller_item_2020;@mueller_iarm_2022] and/or item-restscore metrics [@kreiner_note_2011;@mueller_iarm_2022].\n\nSteps three and four were iterated over, using resampling with replacement from the estimated theta values as a basis for simulating the response data in step three.\n\nSummary statistics were created with focus on the percentage of correct detection of misfit and false positives.\n\nA complete list of software used for the analyses is listed in #sec-addmat.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# read pre-generated item locations for reproducibility\nitems1 <- read_csv(\"data/rm_items40.csv\") %>%\n  slice(1:20) %>% \n  pull(location)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): item, location\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# a matrix to specify which dimension each item loads on\nwmat0 <- matrix(nrow = 20,\n               ncol = 2)\n# set all items to load on dimension one\nwmat0[1:20,1] <- 1\nwmat0[1:20,2] <- 0\n# item 9, with good targeting in this item set (closest to sample theta mean of 0), is chosen to belong to a second dimension\nwmat0[9,1] <- 0\nwmat0[9,2] <- 1\n\n# a matrix to specify which dimension each item loads on\nwmat1 <- matrix(nrow = 20,\n               ncol = 2)\nwmat1[1:20,1] <- 1\nwmat1[1:20,2] <- 0\nwmat1[18,1] <- 0\nwmat1[18,2] <- 1\n\n# a matrix to specify which dimension each item loads on\nwmat2 <- matrix(nrow = 20,\n               ncol = 2)\nwmat2[1:20,1] <- 1\nwmat2[1:20,2] <- 0\nwmat2[13,1] <- 0\nwmat2[13,2] <- 1\n\n# generate dichotomous data\nsimdata0 <- eRm::sim.xdim(10000, items1, cutpoint = \"randomized\", Sigma = sigma, weightmat = wmat0)\nsimdata1 <- eRm::sim.xdim(10000, items1, cutpoint = \"randomized\", Sigma = sigma, weightmat = wmat1)\nsimdata2 <- eRm::sim.xdim(10000, items1, cutpoint = \"randomized\", Sigma = sigma, weightmat = wmat2)\n\nsimdata <- list(data0 = simdata0,\n                data1 = simdata1,\n                data2 = simdata2)\n#saveRDS(simdata,\"simdata10000.rds\")\n\n### And one dataset with all 3 items misfitting\n# a matrix to specify which dimension each item loads on\nwmat3 <- matrix(nrow = 20,\n               ncol = 2)\n# set all items to load on dimension one\nwmat3[1:20,1] <- 1\nwmat3[1:20,2] <- 0\n# item 9, with good targeting in this item set (closest to sample theta mean of 0), is chosen to belong to a second dimension\nwmat3[c(9,13,18),1] <- 0\nwmat3[c(9,13,18),2] <- 1\n\n# generate dichotomous data\nsimdata3 <- eRm::sim.xdim(10000, items1, cutpoint = \"randomized\", Sigma = sigma, weightmat = wmat3)\n\n#saveRDS(simdata3,\"data/simdata3.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# simulate data using 40 items\n# read pre-generated item locations for reproducibility\nitems2 <- read_csv(\"data/rm_items40.csv\") %>%\n  pull(location)\n\n# a matrix to specify which dimension each item loads on\nwmat4 <- matrix(nrow = 40,\n               ncol = 2)\n# set all items to load on dimension one\nwmat4[1:40,1] <- 1\nwmat4[1:40,2] <- 0\n\nwmat4[c(9,13,18),1] <- 0\nwmat4[c(9,13,18),2] <- 1\n\n# generate dichotomous data\nsimdata40items <- eRm::sim.xdim(10000, items2, cutpoint = \"randomized\", Sigma = sigma, weightmat = wmat4)\n\n# check targeting\n#RItargeting(as.data.frame(simdata40items), model = \"RM\")\n#RItargeting(simdata3[,1:10], model = \"RM\")\n\n#saveRDS(simdata40items,\"data/simdata40items.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsimdata <- readRDS(\"data/simdata10000.rds\")\n\nsimdata3 <- readRDS(\"data/simdata3.rds\") %>% \n  as.data.frame()\n\ndemodata <- simdata[[1]] %>% \n  as.data.frame() %>% \n  slice(1:400) %>% \n  select(V1,V11,V3,V12)\n\nsimdata40items <- readRDS(\"data/simdata40items.rds\") %>% \n  as.data.frame()\n```\n:::\n\n\n\n\n\n# Study 1: Item infit and outfit\n\nItem mean square standardized residuals are either unweighted, which is referred to as \"outfit\", or information weighted, which we call \"infit\" [@ostini_polytomous_2006, pp. 86-87]. For details on conditional item fit we refer to the previously mentioned paper by Müller [-@muller_item_2020]. Conditional item infit and outfit are expected to be near 1, with higher values indicating an item to be underfitting the Rasch model (often due to multidimensionality issues) and lower values indicating overfit.\n\nThe function `RIgetfit()` from the `easyRasch` R package is tested here. It's source code can be accessed on GitHub, see #sec-addmat. The function offers the user a choice of the number of bootstrap iterations to use to determine the critical cutoff values for each item's infit and outfit. Our main interest in this study is two-fold. We want to test variations in the number of iterations used in `RIgetfit()` and evaluate how well the critical values based on the parametric bootstrapping procedure detects misfitting items. Additionally, a comparison between infit and outfit statistics in terms of detection rate and false positive rate will be conducted.\n\n20 dichotomous items are used, with one item misfitting. Item locations are the same throughout all studies unless otherwise noted. The location of the misfitting item relative the to the sample theta mean was selected to be approximately 0, -1, and -2 logits. Three separate datasets were generated with these variations, each with 10 000 simulated respondents. One dataset with all three misfitting items was also generated, using the same sample size.\n\nThen the `RIitemfit()` function is used to summarize the bootstrap results and also calculates the infit and outfit for each item in the observed data and highlights items with infit/outfit values outside of the cutoff values. `RIitemfit()` has a default (user modifiable) setting to slightly truncate the distribution of values using `stats::quantile()` at 0.001 and 0.999 to remove extreme values. An example is demonstrated in @tbl-itemfit1, using a subset of the items used in the simulations. @fig-itemfit1 provides a visualization of the distribution of bootstrapped infit and outfit values, together with the infit/outfit values from the observed data illustrated using an orange diamond shape. Note the variation between items in plausible values of infit and outfit based on the bootstrap, and that Smith's rule-of-thumb regarding infit (1±2/√n) would be 0.9-1.1 for a sample size of 400.\n\nThis study was rather computationally demanding since each simulation run entailed 100-400 underlying bootstrap iterations. The sample sizes used were 150, 250, 500, and 1000. The number of iterations to determine cutoff values were 100, 200, and 400. Sample size and iteration conditions were fully crossed with each other and the three different targeting variations of the one misfitting item, resulting in 4*3*3 = 36 conditions. Each combination used 200 simulation runs. The simulations took about 12 hours to run on a Macbook Pro Max M1 using 9 CPU cores.\n\n\n\n\n\n::: {#tbl-itemfit1 .cell tbl-cap='Conditional item fit with simulation based cutoff values'}\n\n```{.r .cell-code .hidden}\nsimfit <- RIgetfit(demodata, iterations = 400, cpu = 8)\nRIitemfit(demodata, simfit, output = \"quarto\")\n```\n\n::: {.cell-output-display}\n\n\n|Item | InfitMSQ|Infit thresholds | OutfitMSQ|Outfit thresholds |Infit diff |Outfit diff | Location|\n|:----|--------:|:----------------|---------:|:-----------------|:----------|:-----------|--------:|\n|V1   |    1.017|[0.828, 1.123]   |     1.061|[0.57, 1.507]     |no misfit  |no misfit   |    -1.37|\n|V11  |    1.000|[0.793, 1.188]   |     1.032|[0.752, 1.315]    |no misfit  |no misfit   |    -0.66|\n|V3   |    1.022|[0.908, 1.114]   |     1.050|[0.63, 1.641]     |no misfit  |no misfit   |     0.46|\n|V12  |    0.966|[0.809, 1.151]   |     0.793|[0.739, 1.206]    |no misfit  |no misfit   |     1.58|\n\n\n:::\n:::\n\n::: {#cell-fig-itemfit1 .cell}\n\n```{.r .cell-code .hidden}\nRIgetfitPlot(simfit, demodata)\n```\n\n::: {.cell-output-display}\n![Distribution of simulation based item fit and estimated item fit from observed data](index_files/figure-docx/fig-itemfit1-1.png){#fig-itemfit1}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# define function to run simulations for item infit/outfit cutoff values\nitemfitboot <- function(dat, iterations, samplesize) {\n  \n  fit <- list()\n  fit <- foreach(i = 1:iterations) %do% {\n    data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n      as.data.frame()\n    \n    # check data for responses in all cells\n    n_resp <-\n      data %>%\n      as.matrix() %>%\n      colSums2() %>%\n      t() %>%\n      as.vector()\n    \n    if (min(n_resp, na.rm = TRUE) < 11) {\n      data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n        as.data.frame()\n    } \n    \n    # get simulation based cutoff values\n    sfit100 <- RIgetfit(data,100,9)\n    sfit200 <- RIgetfit(data,200,9)\n    sfit400 <- RIgetfit(data,400,9)\n\n    # apply cutoffs and store results\n    rfit100 <- RIitemfit(data,sfit100, output = \"dataframe\") %>% \n      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% \n      add_column(item = 1:ncol(data),\n                 sims = 100,\n                 iteration = i,\n                 samplesize = samplesize)\n    \n    rfit200 <- RIitemfit(data,sfit200, output = \"dataframe\") %>% \n      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% \n      add_column(item = 1:ncol(data),\n                 sims = 200,\n                 iteration = i,\n                 samplesize = samplesize)\n    \n    rfit400 <- RIitemfit(data,sfit400, output = \"dataframe\") %>% \n      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% \n      add_column(item = 1:ncol(data),\n                 sims = 400,\n                 iteration = i,\n                 samplesize = samplesize)\n    \n    # combine output \n    fit <- rbind(rfit100,rfit200,rfit400)\n  }\n  return(fit)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsamplesizes <- c(150,250,500,1000)\n\nifb0 <- list()\nifb1 <- list()\nifb2 <- list()\n\n#library(tictoc)\n#tic()\nifb0 <- map(samplesizes, ~ itemfitboot(simdata[[1]], iterations = 200, samplesize = .x))\n#toc() # 14257.358 sec elapsed\n\nifb1 <- map(samplesizes, ~ itemfitboot(simdata[[2]], iterations = 200, samplesize = .x))\nifb2 <- map(samplesizes, ~ itemfitboot(simdata[[3]], iterations = 200, samplesize = .x))\n\n# saveRDS(ifb0, \"data/ifb0_200.rds\")\n# saveRDS(ifb2, \"data/ifb2_200.rds\")\n# saveRDS(ifb1, \"data/ifb1_200.rds\")\n\nifb0_df <- map_dfr(1:4, ~ do.call(\"rbind\", ifb0[[.x]])) %>% \n  add_column(targeting = 0)\n\nifb1_df <- map_dfr(1:4, ~ do.call(\"rbind\", ifb1[[.x]])) %>% \n  add_column(targeting = 1)\n\nifb2_df <- map_dfr(1:4, ~ do.call(\"rbind\", ifb2[[.x]])) %>% \n  add_column(targeting = 2)\n\nifb <- rbind(ifb0_df,ifb1_df,ifb2_df)\n\nwrite_parquet(ifb,\"data/ifb.parquet\")\n```\n:::\n\n\n\n\n\n\n## Results\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nifb <- read_parquet(\"data/ifb.parquet\")\n```\n:::\n\n\n\n\n\nFigures show the percent of simulation runs that have identified an item as misfitting. Items with more than 5% are colored in light red. A number representing the detection rate is shown adjacent to the bar representing the misfitting item. The figure grid columns are labelled with the number of iterations used by `RIgetfit()` to determine cutoff values, and grid rows are labelled with the sample size.\n\n### Infit\n\n\n\n\n\n::: {#cell-fig-ifb0 .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!infit_diff == \"no misfit\",\n         targeting == 0) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n  ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting 0 logits (item 9 misfit). 200 simulations per combination.\",\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional infit detection rate (misfit item at 0 logits)](index_files/figure-docx/fig-ifb0-1.png){#fig-ifb0}\n:::\n:::\n\n\n\n\n\n@fig-ifb0 shows the detection rate when the misfitting item is located at the sample mean. Detection rate is highest for the condition with 100 iterations with sample size 100 and 250, but it also shows higher levels of false positives when sample size increases to 500 or more.\n\n\n\n\n\n::: {#cell-fig-ifb1 .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!infit_diff == \"no misfit\",\n         targeting == 1) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n  ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = 1.2, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -1 logits (item 18 misfit). 200 simulations per combination.\",\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional infit detection rate (misfit item at -1 logits)](index_files/figure-docx/fig-ifb1-1.png){#fig-ifb1}\n:::\n:::\n\n\n\n\n\nWhen the misfitting item is offset in targeting by -1 logits compared to the sample mean (see @fig-ifb1), the smallest sample size has less power to detect misfit compared to the on-target misfitting item. There are lower rates of false positives across all sample sizes and iterations.\n\n\n\n\n\n::: {#cell-fig-ifb2 .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!infit_diff == \"no misfit\",\n         targeting == 2) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n  ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = 1.2, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -2 logits (item 13 misfit). 200 simulations per combination.\",\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional infit detection rate (misfit item at -2 logits)](index_files/figure-docx/fig-ifb2-1.png){#fig-ifb2}\n:::\n:::\n\n\n\n\n\nFinally, when the misfitting item is located at -2 logits compared to the sample mean (see @fig-ifb2), we see a stronger reduction in power for sample sizes 150 and 250. No false positives are identified.\n\n### Outfit\n\n\n\n\n\n::: {#cell-fig-ifb0out .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!outfit_diff == \"no misfit\",\n         targeting == 0) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n    ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = 0, vjust = -1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting 0 logits (item 9 misfit). 200 simulations per combination.\",\n       title = \"Conditional outfit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional outfit detection rate (misfit item at 0 logits)](index_files/figure-docx/fig-ifb0out-1.png){#fig-ifb0out}\n:::\n:::\n\n::: {#cell-fig-ifb1out .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!outfit_diff == \"no misfit\",\n         targeting == 1) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n    ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = 1.2, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -1 logits (item 18 misfit). 200 simulations per combination.\",\n       title = \"Conditional outfit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional outfit detection rate (misfit item at -1 logits)](index_files/figure-docx/fig-ifb1out-1.png){#fig-ifb1out}\n:::\n:::\n\n::: {#cell-fig-ifb2out .cell}\n\n```{.r .cell-code .hidden}\nifb %>% \n  group_by(targeting, samplesize, sims) %>% \n  filter(!outfit_diff == \"no misfit\",\n         targeting == 2) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/200*100) %>% \n    ungroup() %>% \n  mutate(sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                       labels = c(\"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -2 logits (item 13 misfit). 200 simulations per combination.\",\n       title = \"Conditional outfit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional outfit detection rate (misfit item at -2 logits)](index_files/figure-docx/fig-ifb2out-1.png){#fig-ifb2out}\n:::\n:::\n\n\n\n\n\nAs shown in @fig-ifb0out, @fig-ifb1out, and @fig-ifb2out, outfit is performing worse than infit across the board.\n\n### Comments\n\nBased on these simulation, it is highly recommended to use infit over outfit in assessing item fit. The performance of outfit calls to question whether it is useful at all for detecting item misfit.\n\nRegarding infit and the use of parametric bootstrapping with the function `RIgetfit()`, it looks like 100 iterations are to recommend to determine cutoff values when the sample size is 250 or lower, while 200 or 400 iterations reduce the risk for false positives at sample sizes of 500 or larger. False positives are found at sample sizes 500 and 1000 only. The risk for false positives is notably higher when the misfitting item is located at the sample mean compared to when the misfitting item is off-target by -1 logits or more.\n\n\n# Study 2: Item-restscore\n\nItem-restscore is a metric that compares an expected correlation with the observed correlation, using Goodman and Kruskal’s $\\gamma$ [@goodman_measures_1954;@kreiner_note_2011]. Lower observed values than expected indicates than an item is underfit to the Rasch model, while higher values indicate overfit. The item-restscore function used in this simulation is from the `iarm` package [@mueller_iarm_2022] and outputs Benjamini-Hochberg corrected *p*-values [@benjamini_controlling_1995], which are used to determine whether the differences between the observed and expected values are statistically significant (using *p* < .05 as critical value) for each item.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nir <- function(dat, iterations, samplesize, cpu = 9) {\n  \n  require(doParallel)\n  registerDoParallel(cores = cpu)\n  \n  fit <- data.frame()\n  fit <- foreach(i = 1:iterations, .combine = rbind) %dopar% {\n    data <- dat[sample(1:nrow(dat), samplesize), ]\n    \n    erm_out <- RM(data)\n    \n    cfit <- out_infit(erm_out)\n    cfit_d <- data.frame(infit = cfit$Infit, outfit = cfit$Outfit) %>% \n      round(3)\n    \n    i1 <- item_restscore(erm_out)\n    i1 <- as.data.frame(i1)\n    \n    i1d <- data.frame(\"observed\" = as.numeric(i1[[1]][1:ncol(data),1]),\n                     \"expected\" = as.numeric(i1[[1]][1:ncol(data),2]),\n                     \"se\" = as.numeric(i1[[1]][1:ncol(data),3]),\n                     \"p.value\" = as.numeric(i1[[1]][1:ncol(data),4]),\n                     \"p.adj.BH\" = as.numeric(i1[[1]][1:ncol(data),5])\n    ) %>% \n      mutate(diff_abs = abs(expected - observed),\n             diff = expected - observed,\n             ir_padj = ifelse(p.adj.BH < .05, \"sign. misfit\",\"no misfit\")) %>% \n      select(ir_padj, diff, diff_abs) %>% \n      mutate(item = 1:ncol(data)) %>% \n      add_column(iteration = i,\n                 samplesize = samplesize)\n    cbind(i1d,cfit_d)\n  }\n  return(fit)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsamplesizes <- c(100,150,250,500,1000)\n\nir0 <- list()\nir1 <- list()\nir2 <- list()\n#library(tictoc)\n#tic()\nir0 <- map(samplesizes, ~ ir(simdata[[1]], iterations = 1000, samplesize = .x))\nir1 <- map(samplesizes, ~ ir(simdata[[2]], iterations = 1000, samplesize = .x))\nir2 <- map(samplesizes, ~ ir(simdata[[3]], iterations = 1000, samplesize = .x))\n#toc()\n# 483.373 sec elapsed\n\nirall <- list(ir0,ir1,ir2)\n\n#saveRDS(irall,\"data/item_restscore1000.rds\")\n```\n:::\n\n\n\n\n\n## Results\n\n\n\n\n::: {#cell-fig-itemrestscore1 .cell}\n\n```{.r .cell-code .hidden}\nir_all <- readRDS(\"data/item_restscore1000.rds\")\nir_results <- bind_rows(ir_all[[1]],ir_all[[2]],ir_all[[3]]) %>% \n  add_column(targeting = rep(c(0,1,2), each = 100000))\n\n# using patchwork and separate plots to get geom_text working across targeting variations\nresults0 <- ir_results %>% \n  filter(targeting == 0) %>% \n  group_by(targeting, samplesize) %>% \n  filter(ir_padj == \"sign. misfit\" ) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/1000,\n         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','-1 logit offset','-2 logits offset')),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"lightblue\",\"lightpink\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting 0 logits\") +\n  theme(strip.text = element_blank())\n\nresults1 <- ir_results %>% \n  filter(targeting == 1) %>% \n  group_by(targeting, samplesize) %>% \n  filter(ir_padj == \"sign. misfit\" ) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/1000,\n         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','1 logit offset','2 logits offset')),\n         item = factor(item)) %>%  \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"lightblue\",\"lightpink\"))) +\n  geom_text(data = . %>% filter(item == 18),\n            aes(label = n/10), position = position_dodge(width = 0.9),\n            hjust = 1.2, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -1 logits\") +\n  theme(strip.text = element_blank())\n\nresults2 <- ir_results %>% \n  filter(targeting == 2) %>% \n  group_by(targeting, samplesize) %>% \n  filter(ir_padj == \"sign. misfit\" ) %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/1000,\n         targeting = factor(targeting, levels = c(0,1,2), labels = c('On target','1 logit offset','2 logits offset'))) %>% \n  mutate(samplesize = factor(samplesize, levels = c(100,150,250,500,1000),\n                       labels = c(\"n = 100\",\n                                  \"n = 150\",\n                                  \"n = 250\",\n                                  \"n = 500\",\n                                  \"n = 1000\")),\n         item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"lightblue\",\"lightpink\"))) +\n  geom_text(data = . %>% filter(item == 13),\n            aes(label = n/10), position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Targeting -2 logits\")\n\nresults0 + results1 + results2 +\n  plot_layout(axes = \"collect\",\n              axis_titles = \"collect\") +\n  plot_annotation(title = \"Item-restscore detection rate across targeting and sample size\",\n                  subtitle = \"1000 simulated datasets for each combination.\",\n                  theme = theme_rise(fontfamily = \"sans\"))\n```\n\n::: {.cell-output-display}\n![Item-restscore detection rate across targeting and sample size](index_files/figure-docx/fig-itemrestscore1-1.png){#fig-itemrestscore1}\n:::\n:::\n\n\n\n\n\nThis simulation includes an additional condition with 100 respondents, which results in significantly lower detection rates compared to n = 150. Compared to infit at 250 respondents, item-restscore has detection rates of 95.2%, 90.9%, and 62.4% for targeting 0, -1, and -2, while infit has 96.5%, 96.5%, and 71%. For sample size 500 and 1000, detection rate is similar, including the increased tendency for false positives at n = 1000. The false positive rate is lower for item-restscore than infit for sample sizes below 1000.\n\n# Study 3: Comparing infit and item-restscore\n\nWe will now compare the performance of infit and item-restscore when all three items are misfitting at the same time. This simulation will also include a condition with 2000 respondents, to examine if the false positive rate increases with more respondents. For infit, we will only use 200 iterations with `RIgetfit()` since that condition seemed to strike a balance between detection rate and false positives. Outfit is also included to see if it performs as bad as with one misfitting item.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# define function to run simulations for item infit/outfit cutoff values\nitemfitboot2 <- function(dat, iterations, samplesize) {\n  \n  fit <- list()\n  fit <- foreach(i = 1:iterations) %do% {\n    data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n      as.data.frame()\n    \n    # check data for responses in all cells\n    n_resp <-\n      data %>%\n      as.matrix() %>%\n      colSums2() %>%\n      t() %>%\n      as.vector()\n    \n    if (min(n_resp, na.rm = TRUE) < 11) {\n      data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n        as.data.frame()\n    } \n    \n    sfit200 <- RIgetfit(data,200,9)\n\n    # apply cutoffs and store results\n    rfit200 <- RIitemfit(data,sfit200, output = \"dataframe\") %>% \n      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% \n      add_column(item = 1:ncol(data),\n                 sims = 200,\n                 iteration = i,\n                 samplesize = samplesize)\n\n    \n    # combine output \n    fit <- rfit200\n  }\n  return(fit)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsamplesizes <- c(150,250,500,1000,2000)\nifb3 <- list()\nir3 <- list()\n\nifb3 <- map(samplesizes, ~ itemfitboot2(simdata3, iterations = 500, samplesize = .x))\n\nir3 <- map(samplesizes, ~ ir(simdata3, iterations = 500, samplesize = .x))\n\n#saveRDS(ifb3,\"data/ifb3.rds\")\n#saveRDS(ir3,\"data/ir3.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nifb3 <- readRDS(\"data/ifb3.rds\")\nir3 <- readRDS(\"data/ir3.rds\")\n\nifb3_df <- bind_rows(ifb3)\nir3_df <- bind_rows(ir3)\n```\n:::\n\n\n\n\n\n### Results\n\n\n\n\n\n::: {#cell-fig-ifb3out .cell}\n\n```{.r .cell-code .hidden}\nifb3_df %>% \n  group_by(samplesize, sims) %>% \n  filter(!outfit_diff == \"no misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/500*100) %>% \n  ungroup() %>% \n  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\",\n                                        \"n = 2000\")),\n         sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         item = factor(item)) %>% \n\n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = -0.5) +\n    geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n    geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~sims) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(str_wrap(\"3 items misfitting. 200 iterations used to determine critial value. 500 simulations per combination.\",68),\n       title = \"Conditional outfit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional outfit detection rate with three misfitting items](index_files/figure-docx/fig-ifb3out-1.png){#fig-ifb3out}\n:::\n:::\n\n::: {#cell-fig-ifb3 .cell}\n\n```{.r .cell-code .hidden}\nifb3_df %>% \n  group_by(samplesize, sims) %>% \n  filter(!infit_diff == \"no misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n/500*100) %>% \n  ungroup() %>% \n  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\",\n                                        \"n = 2000\")),\n         sims = factor(sims, levels = c(100,200,400),\n                       labels = c(\"100 iterations\",\n                                  \"200 iterations\",\n                                  \"400 iterations\")),\n         item = factor(item)) %>% \n\n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n    geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n    geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~.) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = str_wrap(\"3 items misfitting. 200 iterations used to determine critial value. 500 simulations per combination.\",68),\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output-display}\n![Conditional infit detection rate with three misfitting items](index_files/figure-docx/fig-ifb3-1.png){#fig-ifb3}\n:::\n:::\n\n\n\n\n\nLooking at the performance of infit with three misfitting items (@fig-ifb3), we can see that the detection rate is markedly worse for item 13 (targeting -2 logits) in sample sizes 500 and below, compared to when single items were misfitting. The false positive rate has increased for sample size of 1000 and we can see it increase strongly at n = 2000. Outfit (@fig-ifb3out) again performs worse than infit.\n\n\n\n\n\n::: {#cell-fig-itemrestscore2 .cell}\n\n```{.r .cell-code .hidden}\nir3_df %>% \n  group_by(samplesize) %>% \n  filter(ir_padj == \"sign. misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/500) %>% \n  ungroup() %>% \n  mutate(item = factor(item)) %>% \n    mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\",\n                                        \"n = 2000\"))) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"lightblue\",\"lightpink\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n    geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n    geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.5, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item',  guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"3 items misfitting. 500 simulations per combination.\",\n       title = \"Item-restscore detection rate\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-itemrestscore2-1.png){#fig-itemrestscore2}\n:::\n:::\n\n::: {#cell-fig-comp1 .cell}\n\n```{.r .cell-code .hidden}\nifb3_sum <- ifb3_df %>% \n  group_by(samplesize, sims) %>% \n  filter(!infit_diff == \"no misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(percent = n/500*100) %>% \n  ungroup() %>% \n  filter(item %in% c(9,13,18)) %>% \n  select(!c(n,sims)) %>% \n  add_column(Method = \"Infit\")\n\nir3_sum <- ir3_df %>% \n  group_by(samplesize) %>% \n  filter(ir_padj == \"sign. misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(percent = n*100/500) %>% \n  ungroup() %>% \n  filter(item %in% c(9,13,18)) %>% \n  select(!n) %>% \n  add_column(Method = \"Item-restscore\")\n\nrbind(ifb3_sum,ir3_sum) %>% \n  mutate(item = factor(item, levels = c(9,18,13), labels = c(\"Item 9 (location 0)\",\n                                                          \"Item 18 (location -1)\",\n                                                          \"Item 13 (location -2)\"),\n                       ordered = T)) %>% \n    mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\",\n                                        \"n = 2000\"))) %>% \n\n  ggplot(aes(x = item, y = percent, fill = Method)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = paste0(percent,\"%\"), y = 8), color = \"white\",\n            position = position_dodge(width = 0.9)) +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(y = \"Detection rate in percent\",\n       title = \"Detection rate for item-restscore compared to infit\") +\n  scale_fill_brewer(type = \"qual\", palette = \"Dark2\") +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Detection rate for item-restscore compared to infit](index_files/figure-docx/fig-comp1-1.png){#fig-comp1}\n:::\n:::\n\n\n\n\n\nItem-restscore (see @fig-itemrestscore2) shows comparable detection rate to infit and higher levels of false positives. A comparison is made between the two in @fig-comp1, where item-restscore is performing better than infit at detecting the -2 logits off-target item at n = 250, and better across all items for n = 500 and n = 1000. Infit performs better for samples n = 150 and n = 250 (except the item with location -2 logits).\n\n\n\n\n\n::: {#tbl-overunder .cell tbl-cap='Item-restscore summary results across all sample sizes' tbl-colwidths='[60,40]'}\n\n```{.r .cell-code .hidden}\nir3_df %>%\n  filter(ir_padj == \"sign. misfit\") %>% \n  mutate(type = ifelse(diff < 0, \"overfit\",\"underfit\")) %>% \n  count(item,type) %>% \n  mutate(Percent = n*100/2500) %>% \n  arrange(desc(Percent)) %>% \n  select(!n) %>% \n  set_names(c(\"Item\",\"Type of misfit\",\"Percent\")) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n| Item|Type of misfit | Percent|\n|----:|:--------------|-------:|\n|    9|underfit       |   85.28|\n|   18|underfit       |   78.96|\n|   13|underfit       |   63.80|\n|    2|overfit        |   20.80|\n|    6|overfit        |   19.00|\n|   20|overfit        |   15.48|\n|    8|overfit        |   15.28|\n|   10|overfit        |   14.60|\n|   11|overfit        |   13.04|\n|    7|overfit        |   12.52|\n|   15|overfit        |   12.52|\n|    1|overfit        |   11.96|\n|    5|overfit        |   11.92|\n|   16|overfit        |   11.12|\n|    3|overfit        |    8.96|\n|   14|overfit        |    7.28|\n|    4|overfit        |    7.24|\n|   12|overfit        |    5.96|\n|   17|overfit        |    5.16|\n|   19|overfit        |    1.24|\n|   12|underfit       |    0.08|\n\n\n:::\n:::\n\n\n\n\n\nReviewing the type of misfit identified by item-restscore (see @tbl-overunder), the false positives are all overfitting the Rasch model, except for two instances (out of 2500) indicating underfit for item 12. Items 9, 13, and 18, that were simulated to be misfitting due to loading on a separate dimension, are as expected showing underfit to the Rasch model.\n\n# Study 4: Bootstrapped item-restscore\n\nFor our final set of simulations, we will use a non-parametric bootstrap procedure with item-restscore. The difference from the parametric bootstrap is that the non-parametric bootstrap samples with replacement directly from the observed response data. First, based on the above problematic sample size of 2000 when three items are misfitting, we will use the bootstrap function to sample with replacement using n = 800 and 250 bootstrap samples. The function `RIbootRestscore()` from the `easyRasch` package will be used. \n\n\n\n\n\n::: {#tbl-bootir .cell tbl-cap='Example output from `RIbootRestscore()`' tbl-colwidths='[60,40]'}\n\n```{.r .cell-code .hidden}\n#tic()\nsimdata3 %>% \n  slice_sample(n = 2000) %>% \n  RIbootRestscore(iterations = 250, samplesize = 800, cpu = 8, output = \"dataframe\", cutoff = 0) %>% \n  select(item,item_restscore,percent) %>% \n  filter(!item_restscore == \"no misfit\") %>% \n  arrange(desc(percent)) %>% \n  set_names(c(\"Item\",\"Item-restscore result\",\"Percent of iterations\")) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Item |Item-restscore result | Percent of iterations|\n|:----|:---------------------|---------------------:|\n|V18  |underfit              |                 100.0|\n|V9   |underfit              |                 100.0|\n|V13  |underfit              |                  98.8|\n|V14  |overfit               |                  45.2|\n|V20  |overfit               |                  41.2|\n|V11  |overfit               |                  34.0|\n|V1   |overfit               |                  32.0|\n|V6   |overfit               |                  25.6|\n|V5   |overfit               |                  24.8|\n|V3   |overfit               |                  19.6|\n|V15  |overfit               |                  17.6|\n|V2   |overfit               |                  14.8|\n|V12  |overfit               |                  10.8|\n|V7   |overfit               |                  10.4|\n|V8   |overfit               |                   9.2|\n|V16  |overfit               |                   8.0|\n|V17  |overfit               |                   3.6|\n|V10  |overfit               |                   2.8|\n|V19  |overfit               |                   2.4|\n|V4   |underfit              |                   0.8|\n|V10  |underfit              |                   0.4|\n\n\n:::\n\n```{.r .cell-code .hidden}\n#toc()\n```\n:::\n\n\n\n\n\n`RIbootRestscore()` is demonstrated using a single sample in @tbl-bootir, where the table is sorted on Percent of iterations. The runtime was around 10-12 seconds using 8 CPU cores on a Macbook Pro M1 Max. In our simulation, we will repeat this procedure 500 times and report the average and standard deviation for the percent indicating misfit for each item.\n\nSecond, we will also apply the bootstrapped item-restscore method to sample sizes 150 and 250, using the complete sample for the same bootstrap procedure to see if this produces more useful information than previously tested strategies for identifying misfitting items.\n\n## Results\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nir_boot <- function(dat, iterations, samplesize) {\n  \n  fit <- list()\n  fit <- foreach(i = 1:iterations) %do% {\n    data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n      as.data.frame()\n    \n    fit <- RIbootRestscore(data, iterations = 250, samplesize = 800, cpu = 9, output = \"dataframe\", cutoff = 0) %>% \n      select(item,item_restscore,percent) %>% \n      filter(!item_restscore == \"no misfit\") %>% \n      add_column(iteration = i)\n    \n  }\n  return(fit)\n}\n\nirb0 <- ir_boot(simdata3, 500, 2000)\nsaveRDS(irb0,\"data/irb0.rds\")\n```\n:::\n\n::: {#cell-fig-irb0all .cell}\n\n```{.r .cell-code .hidden}\nirb0 <- readRDS(\"data/irb0.rds\")\n\nirb0_df <- bind_rows(irb0)\n\n# irb0_df %>% \n#   mutate(item = factor(item, levels = paste0(\"V\",20:1), ordered = T)) %>% \n#   ggplot(aes(x = percent, y = item, color = item_restscore)) +\n#   stat_pointinterval(point_interval\t= \"median_hdci\") +\n#   theme_rise(fontfamily = \"sans\") +\n#   labs(caption = str_wrap(\"Point indicates median value and lines are highest-density continuous interval for .66 and .95.\",80),\n#        x = \"Distribution of percent values for item misfit\", y = \"Item\",\n#        title = \"Item-restscore bootstrap results\",\n#        subtitle = str_wrap(\"250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.\",70)\n#        ) +\n#   scale_color_discrete('Type', labels = c(\"Overfit\",\"Underfit\"))\n\nirb0_df %>% \n  mutate(item = factor(item, levels = paste0(\"V\",20:1), ordered = T)) %>% \n  group_by(item,item_restscore) %>% \n  summarise(median = median(percent),\n            mean = mean(percent),\n            q_lower = quantile(percent, .05),\n            q_upper = quantile(percent, .95),\n            sd = sd(percent),\n            mad = mad(percent),\n            se = sd(percent) / sqrt(500),\n            ci_upper = median + (1.96*se),\n            ci_lower = median - (1.96*se)) %>% \n  \n  ggplot(aes(x = median, y = item, color = item_restscore)) +\n  geom_point(size = 3.5, shape = 18) +\n  geom_segment(aes(x = median - mad, xend = median + mad)) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(caption = str_wrap(\"Point indicates median value and horizontal lines are median absolute deviation.\",80),\n       x = \"Distribution of percent values for item misfit\", y = \"Item\",\n       title = \"Item-restscore bootstrap results\",\n       subtitle = str_wrap(\"250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.\",70)\n       ) +\n  scale_color_discrete('Type', labels = c(\"Overfit\",\"Underfit\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'item'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Item-restscore bootstrap results](index_files/figure-docx/fig-irb0all-1.png){#fig-irb0all}\n:::\n:::\n\n::: {#tbl-irb0mis .cell layout-ncol=\"1\" tbl-cap='Summary statistics for item-restscore bootstrap simulation' tbl-subcap='[\"Misfitting items\",\"False positives\"]'}\n\n```{.r .cell-code .hidden}\nirb0_df %>% \n  filter(item %in% c(\"V9\",\"V13\",\"V18\")) %>% \n  rename(Item = item) %>% \n  group_by(Item) %>% \n  summarise(Median = median(percent),\n            MAD = mad(percent),\n            Mean = mean(percent),\n            SD = sd(percent),\n            `Percentile .05` = quantile(percent, .05)\n            ) %>% \n  ungroup() %>% \n  arrange(desc(`Percentile .05`)) %>% \n  mutate_if(is.numeric, round, 1) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Item | Median| MAD|  Mean|  SD| Percentile .05|\n|:----|------:|---:|-----:|---:|--------------:|\n|V9   |  100.0| 0.0| 100.0| 0.1|          100.0|\n|V18  |  100.0| 0.0|  99.8| 0.9|           99.2|\n|V13  |   95.6| 4.7|  93.0| 8.0|           76.4|\n\n\n:::\n\n```{.r .cell-code .hidden}\nirb0_df %>% \n  filter(!item %in% c(\"V9\",\"V13\",\"V18\"),\n         item_restscore == \"overfit\") %>% \n  rename(Item = item) %>% \n  group_by(Item) %>% \n  summarise(Median = median(percent),\n            MAD = mad(percent),\n            Mean = mean(percent),\n            SD = sd(percent),\n            `Percentile .95` = quantile(percent, .95)\n            ) %>% \n  ungroup() %>% \n  arrange(desc(`Percentile .95`)) %>% \n  mutate_if(is.numeric, round, 1) %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Item | Median|  MAD| Mean|   SD| Percentile .95|\n|:----|------:|----:|----:|----:|--------------:|\n|V2   |   20.0| 15.4| 24.3| 16.7|           58.8|\n|V6   |   18.4| 14.2| 21.3| 14.9|           52.8|\n|V8   |   14.4| 12.5| 19.7| 15.6|           51.6|\n|V11  |   14.0| 11.9| 17.6| 13.9|           47.7|\n|V15  |   13.2| 10.1| 16.6| 13.4|           45.2|\n|V20  |   16.4| 13.6| 19.5| 13.9|           45.2|\n|V5   |   12.0| 10.1| 15.7| 12.6|           44.4|\n|V10  |   13.2| 11.3| 16.4| 13.0|           42.4|\n|V16  |   11.4| 10.4| 15.2| 13.0|           42.0|\n|V7   |   13.2| 11.9| 16.7| 12.6|           41.6|\n|V1   |   12.0| 10.1| 15.5| 12.8|           41.3|\n|V3   |   10.8|  9.5| 13.6| 11.2|           36.1|\n|V14  |    7.2|  6.5| 10.4| 10.1|           30.7|\n|V12  |    7.6|  7.7| 10.4|  9.5|           29.7|\n|V4   |    7.2|  6.5| 10.2|  9.5|           29.3|\n|V17  |    6.4|  6.5|  8.4|  8.0|           24.4|\n|V19  |    2.4|  2.4|  3.9|  4.6|           13.6|\n\n\n:::\n:::\n\n\n\n\n\n@fig-irb0all shows that there is variation in false positive rate and it is nearly always indicating overfit, while the misfitting items are only indicated as underfit. The summary statistics in @tbl-irb0mis show that there can be quite a bit of variation for false positives, but the clear majority of results are below 50%. 3 items have 95th percentile values above 50, with the highest at 58.8.\n\n## Small sample (n = 150)\n\nWe will use 200 simulations to check the performance of the bootstrapped item-restscore function for sample size 150. As an additional experimental condition, we will use both 250 and 500 bootstraps for item-restscore in each simulation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nir_boot2 <- function(dat, iterations, samplesize) {\n  \n  fit <- list()\n  fit <- foreach(i = 1:iterations) %do% {\n    data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n      as.data.frame()\n    \n        # check data for responses in all cells\n    n_resp <-\n      data %>%\n      as.matrix() %>%\n      colSums2() %>%\n      t() %>%\n      as.vector()\n    \n    if (min(n_resp, na.rm = TRUE) < 11) {\n      data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n        as.data.frame()\n    } \n    \n    fit1 <- RIbootRestscore(data, iterations = 250, samplesize = nrow(data), cpu = 9, output = \"dataframe\", cutoff = 0) %>% \n      select(item,item_restscore,percent) %>% \n      filter(!item_restscore == \"no misfit\") %>% \n      add_column(iteration = i,\n                 bootit = 250)\n    \n    fit2 <- RIbootRestscore(data, iterations = 500, samplesize = nrow(data), cpu = 9, output = \"dataframe\", cutoff = 0) %>% \n      select(item,item_restscore,percent) %>% \n      filter(!item_restscore == \"no misfit\") %>% \n      add_column(iteration = i,\n                 bootit = 500)\n    \n    fit <- rbind(fit1,fit2)\n    \n  }\n  return(fit)\n}\n\nirb150 <- ir_boot2(simdata3, 200, 150)\nsaveRDS(irb150,\"data/irb150.rds\")\n\n# irb250 <- ir_boot2(simdata3, 200, 250)\n# saveRDS(irb250,\"data/irb250.rds\")\n```\n:::\n\n::: {#cell-fig-irboot150 .cell}\n\n```{.r .cell-code .hidden}\nirb150 <- readRDS(\"data/irb150.rds\")\n\nirb150_df <- bind_rows(irb150)\n\nirb150_df %>% \n  mutate(item = factor(item, levels = paste0(\"V\",20:1), ordered = T),\n         bootit = factor(bootit, levels = c(250,500),\n                         labels = c(\"250 bootstrap iterations\",\n                                    \"500 bootstrap iterations\"))) %>% \n  group_by(item,item_restscore,bootit) %>% \n  summarise(median = median(percent),\n            mean = mean(percent),\n            q_lower = quantile(percent, .05),\n            q_upper = quantile(percent, .95),\n            sd = sd(percent),\n            mad = mad(percent),\n            se = sd(percent) / sqrt(500),\n            ci_upper = median + (1.96*se),\n            ci_lower = median - (1.96*se)) %>% \n  \n  ggplot(aes(x = median, y = item, color = item_restscore)) +\n  geom_point(size = 3.5, shape = 18) +\n  geom_segment(aes(x = median - mad, xend = median + mad)) +\n    geom_text(data = . %>% filter(item == \"V9\",\n                                  item_restscore == \"underfit\"),\n            aes(label = median), color = \"black\",\n            hjust = -0.1, vjust = -1) +\n    geom_text(data = . %>% filter(item == \"V13\",\n                                  item_restscore == \"underfit\"),\n            aes(label = median), color = \"black\",\n            hjust = -0.1, vjust = -1) +\n    geom_text(data = . %>% filter(item == \"V18\",\n                                  item_restscore == \"underfit\"),\n            aes(label = median), color = \"black\",\n            hjust = -0.1, vjust = -1) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(caption = str_wrap(\"Point indicates median value and horizontal lines are median absolute deviation.\",80),\n       x = \"Distribution of percent values for item misfit\", y = \"Item\",\n       title = \"Item-restscore bootstrap results\",\n       subtitle = str_wrap(\"250 bootstrap iterations with 800 respondents from a sample of 2000. 500 simulations were used.\",70)\n       ) +\n  scale_color_discrete('Type', labels = c(\"Overfit\",\"Underfit\")) +\n  facet_wrap(~bootit)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'item', 'item_restscore'. You can override\nusing the `.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# irb150_df %>% \n#   mutate(bootit = factor(bootit, levels = c(250,500),\n#                          labels = c(\"250 bootstrap iterations\",\n#                                     \"500 bootstrap iterations\"))) %>% \n#   ggplot(aes(x = percent, y = item, color = item_restscore)) +\n#   stat_pointinterval(point_interval\t= \"median_hdci\") +\n#   theme_rise(fontfamily = \"sans\") +\n#   labs(caption = str_wrap(\"Point indicates median value and lines are highest-density continuous interval for .66 and .95.\",80),\n#        x = \"Distribution of percent values for item misfit\", y = \"Item\",\n#        title = \"Item-restscore bootstrap results\",\n#        subtitle = str_wrap(\"250 and 500 bootstrap iterations with 150 respondents from a sample of 150. 200 simulations were used.\",70)\n#        ) +\n#   scale_color_discrete('Type', labels = c(\"Overfit\",\"Underfit\")) +\n#   facet_wrap(~bootit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-irboot150-1.png){#fig-irboot150}\n:::\n:::\n\n\n\n\n\nItem-restscore bootstrapping improves slightly on the single instance of item-restscore for the n = 150 condition (see @fig-irboot150). When comparing to the previous results in @fig-itemrestscore2, where the detection rate for the same sample size were at 49.2%, 14.6%, and 34.6% (for items 9, 13, and 18 respectively), the corresponding median values from the bootstrapped item-restscore with 250 iterations were 52.4%, 19.2%, and 38.4%. Using 500 bootstrap iterations did not result in relevant improvements over 250 iterations (see @tbl-irb150mis).\n\n\n\n\n\n::: {#tbl-irb150mis .cell tbl-cap='Summary statistics for item-restscore bootstrap simulation (n = 150)'}\n\n```{.r .cell-code .hidden}\nirb150_tbl <- irb150_df %>% \n  filter(item %in% c(\"V9\",\"V13\",\"V18\"),\n         item_restscore == \"underfit\") %>% \n  rename(Item = item) %>% \n  group_by(Item,bootit) %>% \n  summarise(Median = median(percent),\n            MAD = mad(percent),\n            Mean = mean(percent),\n            SD = sd(percent)\n            ) %>% \n  ungroup() %>% \n  arrange(bootit,desc(Median)) %>% \n  mutate_if(is.numeric, round, 1) %>% \n  mutate(bootit = as.character(bootit)) %>% \n  relocate(bootit, .before = \"Item\") %>% \n  rename(`Bootstrap iterations` = bootit)  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Item'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nirb150_tbl[c(1,3,4,6),1] <- \"\"\n\nknitr::kable(irb150_tbl)\n```\n\n::: {.cell-output-display}\n\n\n|Bootstrap iterations |Item | Median|  MAD| Mean|   SD|\n|:--------------------|:----|------:|----:|----:|----:|\n|                     |V9   |   52.4| 38.0| 51.4| 29.2|\n|250                  |V18  |   38.4| 35.6| 42.0| 27.8|\n|                     |V13  |   19.2| 21.9| 27.3| 25.0|\n|                     |V9   |   54.3| 37.8| 51.0| 29.5|\n|500                  |V18  |   37.2| 35.9| 41.4| 27.9|\n|                     |V13  |   19.4| 23.1| 27.2| 24.7|\n\n\n:::\n:::\n\n\n\n\n\n\n# Study 5: Varying number of items \n\nWhen doing simulation studies there is always a balance to strike between trying to evaluate many scenarios and not having too high complexity. We have been keeping several things constant, such as item locations and number of items, which makes interpretation easier but may limit the applicability of the results. For our final simulation, we will vary the number of items and the number of misfitting items. First, 40 dichotomous items will be used, adding 20 new item locations to the previously used set, with the same three items misfitting (items 9, 13, and 18). Second, items 1-10 out of the initial 20 items will be used, which means only item 9 will be misfit. We'll again be using sample sizes of 150, 250, 500, and 1000.\n\nItem-restscore and item infit will be compared. The latter will use 100 bootstrap iterations to determine critical values for sample sizes 150 and 250, and 200 bootstrap iterations for n >= 500.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nitemfitboot3 <- function(dat, iterations, samplesize) {\n  \n  fit <- list()\n  fit <- foreach(i = 1:iterations) %do% {\n    data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n      as.data.frame()\n    \n    # check data for responses in all cells\n    n_resp <-\n      data %>%\n      as.matrix() %>%\n      colSums2() %>%\n      t() %>%\n      as.vector()\n    \n    if (min(n_resp, na.rm = TRUE) < 11) {\n      data <- dat[sample(1:nrow(dat), samplesize), ] %>% \n        as.data.frame()\n    } \n    \n    if (nrow(data < 400)) {\n      sfit <- RIgetfit(data,100,9)\n    } else if (nrow(data > 400)) {\n      sfit <- RIgetfit(data,200,9)\n    }\n    \n    # apply cutoffs and store results\n    rfit <- RIitemfit(data, sfit, output = \"dataframe\") %>% \n      select(infit_msq,outfit_msq,infit_diff,outfit_diff) %>% \n      add_column(item = 1:ncol(data),\n                 iteration = i,\n                 samplesize = samplesize)\n\n    # combine output \n    fit <- rfit\n  }\n  return(fit)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsamplesizes <- c(150,250,500,1000)\nifb10 <- list()\nir10 <- list()\n\n#tic()\nifb40_60 <- map(samplesizes, ~ itemfitboot3(simdata40items, iterations = 60, samplesize = .x))\n#toc() # 120s for 2 runs\nsaveRDS(ifb40_60, \"data/ifb40_60i.rds\")\n\nifb40_140 <- map(samplesizes, ~ itemfitboot3(simdata40items, iterations = 140, samplesize = .x))\n#toc() # 120s for 2 runs\nsaveRDS(ifb40_140, \"data/ifb40_140i.rds\")\n\nifb10 <- map(samplesizes, ~ itemfitboot3(simdata3[,1:10], iterations = 200, samplesize = .x))\n\nsaveRDS(ifb10, \"data/ifb10.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nir40 <- map(samplesizes, ~ ir(simdata40items, iterations = 500, samplesize = .x))\nsaveRDS(ir40, \"data/ir40.rds\")\n\nir10 <- map(samplesizes, ~ ir(simdata3[,1:10], iterations = 500, samplesize = .x))\nsaveRDS(ir10, \"data/ir10.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nir40 <- readRDS(\"data/ir40.rds\")\nir10 <- readRDS(\"data/ir10.rds\")\n\nifb40_60 <- readRDS(\"data/ifb40_60i.rds\")\nifb40_140 <- readRDS(\"data/ifb40_140i.rds\")\nifb10 <- readRDS(\"data/ifb10.rds\")\n\n\nir40_df <- bind_rows(ir40)\nir10_df <- bind_rows(ir10)\n\nifb40_60_df <- bind_rows(ifb40_60)\nifb40_140_df <- bind_rows(ifb40_140)\nifb40_df <- rbind(ifb40_60_df,ifb40_140_df)\n\nifb10_df <- bind_rows(ifb10)\n```\n:::\n\n\n\n\n\n## Results 40 items\n\n\n\n\n\n::: {#cell-fig-ifb40 .cell}\n\n```{.r .cell-code .hidden}\nifb40_df %>% \n  group_by(samplesize) %>% \n  filter(!infit_diff == \"no misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = round(n/200*100,1)) %>% \n  ungroup() %>% \n  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000,2000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\",\n                                        \"n = 2000\"))) %>% \n\n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n    geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n    geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_continuous('Item', limits = c(1,40), breaks = seq(1,40,1), guide = guide_axis(n.dodge = 2), minor_breaks = NULL) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize~.) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"3 items misfitting. 200 simulations per combination.\",\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_col()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-ifb40-1.png){#fig-ifb40}\n:::\n:::\n\n::: {#cell-fig-itemrestscore40 .cell}\n\n```{.r .cell-code .hidden}\nir40_df %>% \n  group_by(samplesize) %>% \n  filter(ir_padj == \"sign. misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/500) %>% \n  ungroup() %>% \n  mutate(item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"lightblue\",\"lightpink\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.65, vjust = 1) +\n    geom_text(data = . %>% filter(item == 13),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.65, vjust = 1) +\n    geom_text(data = . %>% filter(item == 18),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.65, vjust = 1) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item', \n                     guide = guide_axis(n.dodge = 2)) +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"3 items misfitting. 500 simulations per combination.\",\n       title = \"Item-restscore detection rate\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-itemrestscore40-1.png){#fig-itemrestscore40}\n:::\n:::\n\n\n\n\n\nInfit performs better when sample size is 150 or 250 (see @fig-ifb40), while performance is slightly better for item-restscore for n >= 500 in terms of lower rates of false positives (see @fig-itemrestscore40).\n\n## Results 10 items\n\n\n\n\n\n::: {#cell-fig-ifb10 .cell}\n\n```{.r .cell-code .hidden}\nifb10_df %>% \n  group_by(samplesize) %>% \n  filter(!infit_diff == \"no misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = round(n/200*100,1)) %>% \n  ungroup() %>% \n  mutate(samplesize = factor(samplesize, levels = c(150,250,500,1000),\n                             labels = c(\"n = 150\",\n                                        \"n = 250\",\n                                        \"n = 500\",\n                                        \"n = 1000\"))) %>% \n  mutate(item = factor(item)) %>% \n\n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), \n            position = position_dodge(width = 0.9),\n            hjust = -0.3, vjust = 2) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete('Item') +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  coord_cartesian(clip = \"off\") +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Item 9 misfitting (targeting = 0). 200 simulations per combination.\",\n       title = \"Conditional infit detection rate\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-ifb10-1.png){#fig-ifb10}\n:::\n:::\n\n::: {#cell-fig-itemrestscore10 .cell}\n\n```{.r .cell-code .hidden}\nir10_df %>% \n  group_by(samplesize) %>% \n  filter(ir_padj == \"sign. misfit\") %>% \n  count(item, .drop = F) %>% \n  mutate(Percent = n*100/500) %>% \n  ungroup() %>% \n  mutate(item = factor(item)) %>% \n  \n  ggplot(aes(x = item, y = Percent)) +\n  geom_col(aes(fill = ifelse(Percent > 5, \"a\",\"b\"))) +\n  geom_text(data = . %>% filter(item == 9),\n            aes(label = Percent), position = position_dodge(width = 0.9),\n            hjust = -0.65, vjust = 2) +\n  scale_y_continuous(limits = c(0,105), breaks = seq(0,100,25)) +\n  scale_x_discrete() +\n  guides(fill = \"none\") +\n  facet_grid(samplesize ~ .) +\n  theme_rise(fontfamily = \"sans\") +\n  labs(subtitle = \"Item 9 misfitting (targeting = 0). 500 simulations per combination.\",\n       title = \"Item-restscore detection rate\",\n       x = \"Item\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-itemrestscore10-1.png){#fig-itemrestscore10}\n:::\n:::\n\n\n\n\n\nCompared to when we had 20 items, ... create a summary table! And run more simulations tonight.\n\n\n# Discussion\n\n## Limitations\n\nNumber of items could be varied more. However, the results from Müller [-@muller_item_2020], which use 10, 15, and 20 items, indicates small differences in critical value ranges. But this might not have implications for detection rate of misfitting items (we need the 40 items simulation and maybe 10 also?). Partial credit model for polytomous data would have been nice to also test. Although results regarding detection rate should generalize from RM to PCM, maybe the sample size in relation to number of items does not easily translate from the dichotomous case?\n\n# Conclusion\n\nThese findings make a good argument for removing one item at a time when the analysis indicates misfitting items, starting with the most underfitting item. This is especially relevant for *n* >= 500 and when misfitting items are located close to the sample mean.\n\n\n\n\n\n::: {#cell-fig-loadloc .cell}\n\n```{.r .cell-code .hidden}\nRIloadLoc(simdata3[1:400,], model = \"RM\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/fig-loadloc-1.png){#fig-loadloc}\n:::\n:::\n\n\n\n\n\nAssessing item fit and dimensionality should be done using multiple methods. Item fit and item-restscore should be used in parallel, while also examining residual patterns by reviewing standardized factor loadings on the first residual contrast (see @fig-loadloc for an example) as well as Yen's Q3 residual correlations.\n\nWhat does this look like with a small sample?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# RIloadLoc(simdata3[1:150,], model = \"RM\")\n# simfit2 <- RIgetfit(simdata3[1:150,], iterations = 100, cpu = 8)\n# RIitemfit(simdata3[1:150,], simfit2)\n# RIgetfitPlot(simfit2, simdata3[1:150,])\n```\n:::\n\n\n\n\n\n\nWhile the simulations in this paper have used dichotomous data, all functions evaluated in this paper also work with polytomous data using the Rasch Partial Credit Model.\n\n# References {.unnumbered}\n\n:::{#refs}\n\n:::\n\n# Additional materials {#sec-addmat}\n\n- GitHub link for `easyRasch` source code: <https://github.com/pgmj/easyRasch/>\n  - Most functions are defined in this file: <https://github.com/pgmj/easyRasch/blob/main/R/easyRasch.R>\n\n## Session info\n\nThis documents the specific R packages and versions used in this study. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n [1] parallel  grid      stats4    stats     graphics  grDevices utils    \n [8] datasets  methods   base     \n\nother attached packages:\n [1] showtext_0.9-7    showtextdb_3.0    sysfonts_0.8.9    arrow_16.1.0     \n [5] easyRasch_0.3.3   doParallel_1.0.17 iterators_1.0.14  furrr_0.3.1      \n [9] future_1.34.0     foreach_1.5.2     janitor_2.2.0     hexbin_1.28.4    \n[13] catR_3.17         glue_1.8.0        ggrepel_0.9.6     patchwork_1.3.0  \n[17] reshape_0.8.9     matrixStats_1.4.1 psychotree_0.16-1 psychotools_0.7-4\n[21] partykit_1.2-22   mvtnorm_1.3-1     libcoin_1.0-10    psych_2.4.6.26   \n[25] mirt_1.43         lattice_0.22-6    kableExtra_1.4.0  formattable_0.2.1\n[29] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[33] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[37] tidyverse_2.0.0   ggdist_3.3.2      iarm_0.4.3        ggplot2_3.5.1    \n[41] eRm_1.0-6        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2        R.oo_1.26.0          cellranger_1.1.0    \n  [4] rpart_4.1.23         lifecycle_1.0.4      rprojroot_2.0.4     \n  [7] globals_0.16.3       vroom_1.6.5          MASS_7.3-61         \n [10] backports_1.5.0      magrittr_2.0.3       vcd_1.4-12          \n [13] Hmisc_5.2-0          rmarkdown_2.28       yaml_2.3.10         \n [16] sessioninfo_1.2.2    pbapply_1.7-2        RColorBrewer_1.1-3  \n [19] audio_0.1-11         quadprog_1.5-8       R.utils_2.12.3      \n [22] nnet_7.3-19          listenv_0.9.1        testthat_3.2.1.1    \n [25] RPushbullet_0.3.4    vegan_2.6-8          parallelly_1.38.0   \n [28] svglite_2.1.3        permute_0.9-7        codetools_0.2-20    \n [31] xml2_1.3.6           tidyselect_1.2.1     farver_2.1.2        \n [34] base64enc_0.1-3      jsonlite_1.8.9       progressr_0.14.0    \n [37] Formula_1.2-5        survival_3.7-0       systemfonts_1.1.0   \n [40] tools_4.4.2          gnm_1.1-5            snow_0.4-4          \n [43] Rcpp_1.0.13-1        mnormt_2.1.1         gridExtra_2.3       \n [46] xfun_0.46            here_1.0.1           mgcv_1.9-1          \n [49] distributional_0.4.0 ca_0.71.1            withr_3.0.2         \n [52] beepr_2.0            fastmap_1.2.0        fansi_1.0.6         \n [55] digest_0.6.37        timechange_0.3.0     R6_2.5.1            \n [58] colorspace_2.1-1     R.methodsS3_1.8.2    inum_1.0-5          \n [61] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [64] SimDesign_2.17.1     htmlwidgets_1.6.4    pkgconfig_2.0.3     \n [67] gtable_0.3.5         lmtest_0.9-40        brio_1.1.5          \n [70] htmltools_0.5.8.1    scales_1.3.0         snakecase_0.11.1    \n [73] knitr_1.48           rstudioapi_0.17.1    tzdb_0.4.0          \n [76] checkmate_2.3.2      nlme_3.1-166         curl_6.0.1          \n [79] zoo_1.8-12           relimp_1.0-5         vcdExtra_0.8-5      \n [82] foreign_0.8-87       pillar_1.9.0         vctrs_0.6.5         \n [85] Deriv_4.1.3          cluster_2.1.6        dcurver_0.9.2       \n [88] archive_1.1.8        GPArotation_2024.3-1 htmlTable_2.4.3     \n [91] evaluate_1.0.1       cli_3.6.3            compiler_4.4.2      \n [94] rlang_1.1.4          crayon_1.5.3         future.apply_1.11.2 \n [97] labeling_0.4.3       plyr_1.8.9           stringi_1.8.4       \n[100] viridisLite_0.4.2    assertthat_0.2.1     munsell_0.5.1       \n[103] Matrix_1.7-1         qvcalc_1.0.3         hms_1.1.3           \n[106] bit64_4.0.5          bit_4.0.5            readxl_1.4.3        \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}